<p align="center">
  <img src="https://raw.githubusercontent.com/orneryd/NornicDB/refs/heads/main/docs/assets/logos/nornicdb-logo.svg" alt="NornicDB Logo" width="200"/>
</p>

<h1 align="center">NornicDB</h1>

<p align="center">
  <strong>The Graph Database That Learns</strong><br/>
  Neo4j-compatible â€¢ GPU-accelerated â€¢ Memory that evolves
</p>

<p align="center">
  <img src="https://img.shields.io/badge/version-1.0.0-success" alt="Version 1.0.0">
  <a href="https://github.com/orneryd/NornicDB"><img src="https://img.shields.io/badge/github-orneryd%2FNornicDB-blue?logo=github" alt="GitHub"></a>
  <a href="https://hub.docker.com/u/timothyswt"><img src="https://img.shields.io/badge/docker-ready-blue?logo=docker" alt="Docker"></a>
  <a href="https://neo4j.com/"><img src="https://img.shields.io/badge/neo4j-compatible-008CC1?logo=neo4j" alt="Neo4j Compatible"></a>
  <a href="https://go.dev/"><img src="https://img.shields.io/badge/go-%3E%3D1.26-00ADD8?logo=go" alt="Go Version"></a>
  <a href="LICENSE"><img src="https://img.shields.io/badge/license-MIT-blue" alt="License"></a>
</p>

<p align="center">
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#what-problem-does-this-solve">Problem</a> â€¢
  <a href="#why-nornicdb-is-different">Why Different</a> â€¢
  <a href="#performance-snapshot">Benchmarks</a> â€¢
  <a href="#features">Features</a> â€¢
  <a href="#docker-images">Docker</a> â€¢
  <a href="#documentation">Docs</a> â€¢
  <a href="#contributors">Contributors</a>
</p>

---

## What Problem Does This Solve?

NornicDB is a high-performance graph database designed for AI agents and knowledge systems. It speaks Neo4j's language (Bolt protocol + Cypher) so you can switch with zero code changes, while adding intelligent features that traditional databases lack.

NornicDB automatically discovers and manages relationships in your data, weaving connections that let meaning emerge from your knowledge graph.

## Why NornicDB Is Different

- **Neo4j-compatible by default**: Bolt + Cypher support for existing drivers and applications.
- **Built for AI-native workloads**: vector search, memory decay, and auto-relationships are first-class features.
- **Hardware-accelerated execution**: Metal/CUDA/Vulkan pathways for high-throughput graph + semantic workloads.
- **Operational flexibility**: full images (models included), BYOM images, and headless API-only deployments.
- **Canonical graph ledger support**: versioned facts, temporal validity, as-of reads, queryable txlog, and receipts for audit-oriented systems.

### What Recent Deep-Dives Show

- **Hybrid execution model (streaming fast paths + general engine)**: NornicDB uses shape-specialized streaming executors for common traversal/aggregation patterns while retaining a general Cypher path for coverage and correctness.
- **Runtime parser mode switching**: the default `nornic` mode minimizes hot-path overhead, while `antlr` mode prioritizes strict parsing and diagnostics when debugging/query validation matters.
- **Measured parser-path deltas on benchmark suites**: internal Northwind comparisons show large overhead differences on certain query shapes when full parse-tree paths are used, which is why default production mode is optimized for lower overhead.
- **HNSW build acceleration from insertion-order optimization**: BM25-seeded insertion order reduced a 1M embedding build from ~27 minutes to ~10 minutes (~2.7x) in published tests by reducing traversal waste during construction, without changing core quality knobs.
- **Shared seed strategy across indexing stages**: the same lexical seed extraction supports HNSW insertion ordering and improves k-means centroid initialization spread for vector pipeline efficiency.

Read more:
- [Cypher parser modes and execution trade-offs](docs/architecture/cypher-parser-modes.md)
- [How we sped up HNSW construction 2.7x](https://dev.to/orneryd/how-i-sped-up-hnsw-construction-27x-2jhn)

## Performance Snapshot

**LDBC Social Network Benchmark** (M3 Max, 64GB):

| Query Type                    | NornicDB      | Neo4j       | Speedup |
| ----------------------------- | ------------- | ----------- | ------- |
| **Message content lookup**    | 6,389 ops/sec | 518 ops/sec | **12x** |
| **Recent messages (friends)** | 2,769 ops/sec | 108 ops/sec | **25x** |
| **Avg friends per city**      | 4,713 ops/sec | 91 ops/sec  | **52x** |
| **Tag co-occurrence**         | 2,076 ops/sec | 65 ops/sec  | **32x** |

> See [full benchmark results](docs/performance/benchmarks-vs-neo4j.md) for complete methodology and additional workloads.

## Quick Start

### Docker (Recommended)

```bash
# Apple Silicon (includes bge-m3 embedding model)
docker run -d --name nornicdb \
  -p 7474:7474 -p 7687:7687 \
  -v nornicdb-data:/data \
  timothyswt/nornicdb-arm64-metal-bge:latest  # Apple Silicon
  # timothyswt/nornicdb-amd64-cuda-bge:latest  # NVIDIA GPU
```

Open [http://localhost:7474](http://localhost:7474) for the admin UI.

Need a different image/profile (Heimdall, BYOM, CPU-only, Vulkan, headless)?
- [Docker image quick reference](docs/getting-started/image-quick-reference.md)
- [Docker images section](#docker-images)

### From Source

```bash
git clone https://github.com/orneryd/NornicDB.git
cd NornicDB
go build -o nornicdb ./cmd/nornicdb
./nornicdb serve
```

### Connect

Use any Neo4j driver â€” Python, JavaScript, Go, Java, .NET:

```python
from neo4j import GraphDatabase

driver = GraphDatabase.driver("bolt://localhost:7687")
with driver.session() as session:
    session.run("CREATE (n:Memory {content: 'Hello NornicDB'})")
```

## Build It Yourself

Detailed local build, cross-compile, and packaging instructions:
- [DIY instructions](DIY.md)
- [Building section](#building)

## Features

### ðŸ”Œ Neo4j Compatible

Drop-in replacement for Neo4j. Your existing code works unchanged.

- **Bolt Protocol** â€” Use official Neo4j drivers
- **Cypher Queries** â€” Full query language support
- **Schema Management** â€” Constraints, indexes, vector indexes
- **Qdrant gRPC API Compatible** â€” Works with Qdrant-style gRPC vector workflows

### ðŸ§  Intelligent Memory

Memory that behaves like human cognition.

| Memory Tier    | Half-Life | Use Case               |
| -------------- | --------- | ---------------------- |
| **Episodic**   | 7 days    | Chat context, sessions |
| **Semantic**   | 69 days   | Facts, decisions       |
| **Procedural** | 693 days  | Skills, patterns       |

```cypher
// Find memories that are still strong
MATCH (m:Memory) WHERE m.decayScore > 0.5
RETURN m.title ORDER BY m.decayScore DESC
```

### ðŸ”— Auto-Relationships

NornicDB weaves connections automatically:

- **Embedding Similarity** â€” Related concepts link together
- **Co-access Patterns** â€” Frequently queried pairs connect
- **Temporal Proximity** â€” Same-session nodes associate
- **Transitive Inference** â€” Aâ†’B + Bâ†’C suggests Aâ†’C

### âš¡ Performance

**LDBC Social Network Benchmark** (M3 Max, 64GB):

| Query Type                    | NornicDB      | Neo4j       | Speedup |
| ----------------------------- | ------------- | ----------- | ------- |
| **Message content lookup**    | 6,389 ops/sec | 518 ops/sec | **12x** |
| **Recent messages (friends)** | 2,769 ops/sec | 108 ops/sec | **25x** |
| **Avg friends per city**      | 4,713 ops/sec | 91 ops/sec  | **52x** |
| **Tag co-occurrence**         | 2,076 ops/sec | 65 ops/sec  | **32x** |

**Northwind Benchmark** (M3 Max vs Neo4j, same hardware):

| Operation        | NornicDB      | Neo4j         | Speedup  |
| ---------------- | ------------- | ------------- | -------- |
| **Index lookup** | 7,623 ops/sec | 2,143 ops/sec | **3.6x** |
| **Count nodes**  | 5,253 ops/sec | 798 ops/sec   | **6.6x** |
| **Write: node**  | 5,578 ops/sec | 1,690 ops/sec | **3.3x** |
| **Write: edge**  | 6,626 ops/sec | 1,611 ops/sec | **4.1x** |

**Cross-Platform (CUDA on Windows i9-9900KF + RTX 2080 Ti):**

| Operation                 | Throughput    |
| ------------------------- | ------------- |
| **Orders by customer**    | 4,252 ops/sec |
| **Products out of stock** | 4,174 ops/sec |
| **Find category**         | 4,071 ops/sec |

**Additional advantages:**

- **Memory footprint**: 100-500 MB vs 1-4 GB for Neo4j
- **Cold start**: <1s vs 10-30s for Neo4j

> See [full benchmark results](docs/performance/benchmarks-vs-neo4j.md) for detailed comparisons.

### ðŸŽ¯ Vector Search

Native semantic search with GPU acceleration. NornicDB automatically indexes all node embeddings - no manual index creation required.

> ðŸ“– **Deep Dive:** See [Vector Search Guide](docs/user-guides/vector-search.md) for internal index architecture, user-defined indexes, and the embedding lookup order.

**Option 1: Vector Array (Neo4j Compatible)**

Provide your own embeddings - works identically to Neo4j:

```cypher
// Query with a pre-computed embedding vector
CALL db.index.vector.queryNodes(
  'embeddings',            // Index name (created via createNodeIndex)
  10,                      // Number of results
  [0.1, 0.2, 0.3, ...]     // Your query vector
) YIELD node, score
RETURN node.content, score
```

**Option 2: String Query (NornicDB Enhanced)**

Let NornicDB handle embedding generation automatically:

```cypher
// Query with natural language - NornicDB generates the embedding
CALL db.index.vector.queryNodes(
  'embeddings',              // Index name
  10,                        // Number of results
  'machine learning guide'   // Natural language query (auto-embedded)
) YIELD node, score
RETURN node.content, score
```

> ðŸ’¡ **Note:** String queries require an embedder to be configured. When enabled, NornicDB automatically generates embeddings server-side using the configured model (Ollama, OpenAI, or local GGUF).

**Option 3: REST API (Hybrid Search)**

Use the REST endpoint for combined vector + BM25 search:

```bash
curl -X POST http://localhost:7474/nornicdb/search \
  -H "Content-Type: application/json" \
  -d '{"query": "machine learning", "limit": 10}'
```

**Option 4: GraphQL API (Hybrid Search)**

Use GraphQL for the same hybrid retrieval via `search(query, options)`:

```bash
curl -X POST http://localhost:7474/graphql \
  -H "Content-Type: application/json" \
  -d '{
    "query": "query Search { search(query: \"machine learning\", options: { limit: 10, method: HYBRID }) { totalCount executionTimeMs results { score rrfScore foundBy node { id labels } } } }"
  }'
```

**Option 5: gRPC API (Qdrant-Compatible Search)**

Use existing Qdrant gRPC clients against NornicDB (`:6334` by default):

```python
from qdrant_client import QdrantClient

client = QdrantClient(host="127.0.0.1", grpc_port=6334, prefer_grpc=True)
results = client.search(
    collection_name="my_vectors",
    query_vector=[1.0] * 128,
    limit=10,
)
print([r.id for r in results])
```

String search is also available using existing Qdrant gRPC driver semantics via `Points.Query` with `Document.text`:

```python
from qdrant_client import QdrantClient, models

client = QdrantClient(host="127.0.0.1", grpc_port=6334, prefer_grpc=True)
resp = client.query_points(
    collection_name="my_vectors",
    query=models.Document(text="machine learning"),
    limit=10,
)
```

NornicDB also exposes an additive native RPC (`NornicSearch/SearchText`) on the same endpoint:

```go
conn, _ := grpc.Dial("127.0.0.1:6334", grpc.WithTransportCredentials(insecure.NewCredentials()))
defer conn.Close()

client := nornicpb.NewNornicSearchClient(conn)
resp, _ := client.SearchText(ctx, &nornicpb.SearchTextRequest{
    Query: "machine learning",
    Limit: 10,
})
_ = resp
```

> Prerequisite: add the Nornic proto-generated client alongside your existing Qdrant driver usage. See `docs/user-guides/nornic-search-grpc.md`.
>
> Enable with `NORNICDB_QDRANT_GRPC_ENABLED=true`. See `docs/user-guides/qdrant-grpc.md` for setup and Qdrant compatibility notes.

### ðŸ¤– Heimdall AI Assistant

Built-in AI that understands your database.

```bash
# Enable Heimdall
NORNICDB_HEIMDALL_ENABLED=true ./nornicdb serve
```

**Natural Language Queries:**
- "Get the database status"
- "Show me system metrics"
- "Run health check"

**Plugin System:**
- Create custom actions the AI can execute
- Lifecycle hooks (PrePrompt, PreExecute, PostExecute)
- Database event monitoring for autonomous actions
- Inline notifications with proper ordering

See [Heimdall AI Assistant Guide](docs/user-guides/heimdall-ai-assistant.md) and [Plugin Development](docs/user-guides/heimdall-plugins.md).

### ðŸ§© APOC Functions

950+ built-in functions for text, math, collections, and more. Plus a plugin system for custom extensions.

```cypher
// Text processing
RETURN apoc.text.camelCase('hello world')  // "helloWorld"
RETURN apoc.text.slugify('Hello World!')   // "hello-world"

// Machine learning
RETURN apoc.ml.sigmoid(0)                  // 0.5
RETURN apoc.ml.cosineSimilarity([1,0], [0,1])  // 0.0

// Collections
RETURN apoc.coll.sum([1, 2, 3, 4, 5])      // 15
```

Drop custom `.so` plugins into `/app/plugins/` for automatic loading. See the [APOC Plugin Guide](docs/features/plugin-system.md).

## Docker Images

All images available at [Docker Hub](https://hub.docker.com/u/timothyswt).

### ARM64 (Apple Silicon)

| Image | Size | Description |
|-------|------|-------------|
| `timothyswt/nornicdb-arm64-metal-bge-heimdall` | 1.1 GB | **Full** - Embeddings + AI Assistant |
| `timothyswt/nornicdb-arm64-metal-bge` | 586 MB | **Standard** - With BGE-M3 embeddings |
| `timothyswt/nornicdb-arm64-metal` | 148 MB | **Minimal** - Core database, BYOM |
| `timothyswt/nornicdb-arm64-metal-headless` | 148 MB | **Headless** - API only, no UI |

### AMD64 (Linux/Intel)

| Image | Size | Description |
|-------|------|-------------|
| `timothyswt/nornicdb-amd64-cuda-bge` | ~4.5 GB | **GPU + Embeddings** - CUDA + BGE-M3 |
| `timothyswt/nornicdb-amd64-cuda` | ~3 GB | **GPU** - CUDA acceleration, BYOM |
| `timothyswt/nornicdb-amd64-cuda-headless` | ~2.9 GB | **GPU Headless** - API only |
| `timothyswt/nornicdb-amd64-cpu` | ~500 MB | **CPU** - No GPU required |
| `timothyswt/nornicdb-amd64-cpu-headless` | ~500 MB | **CPU Headless** - API only |

**BYOM** = Bring Your Own Model (mount at `/app/models`)

```bash
# With your own model
docker run -d -p 7474:7474 -p 7687:7687 \
  -v /path/to/models:/app/models \
  timothyswt/nornicdb-arm64-metal:latest

# Headless mode (API only, no web UI)
docker run -d -p 7474:7474 -p 7687:7687 \
  -v nornicdb-data:/data \
  timothyswt/nornicdb-arm64-metal-headless:latest
```

### Headless Mode

For embedded deployments, microservices, or API-only use cases, NornicDB supports headless mode which disables the web UI for a smaller binary and reduced attack surface.

**Runtime flag:**

```bash
nornicdb serve --headless
```

**Environment variable:**

```bash
NORNICDB_HEADLESS=true nornicdb serve
```

**Build without UI (smaller binary):**

```bash
# Native build
make build-headless

# Docker build
docker build --build-arg HEADLESS=true -f docker/Dockerfile.arm64-metal .
```

## Configuration

```yaml
# nornicdb.yaml
server:
  bolt_port: 7687
  http_port: 7474
  host: localhost

database:
  data_dir: ./data
  async_writes_enabled: true
  async_flush_interval: 50ms
  async_max_node_cache_size: 50000
  async_max_edge_cache_size: 100000

embedding:
  enabled: true
  provider: local # or ollama, openai
  model: bge-m3.gguf
  url: ""
  dimensions: 1024

embedding_worker:
  chunk_size: 8192
  chunk_overlap: 50

memory:
  decay_enabled: true
  decay_interval: 1h
  auto_links_enabled: true
  auto_links_similarity_threshold: 0.82
```

## Use Cases

- **AI Agent Memory** â€” Persistent, queryable memory for LLM agents
- **Knowledge Graphs** â€” Auto-organizing knowledge bases
- **RAG Systems** â€” Vector + graph retrieval in one database
- **Graph-RAG for LLM Inference** â€” Simplify retrieval pipelines by combining graph traversal, hybrid search, and provenance in one engine
- **Session Context** â€” Decaying conversation history
- **Research Tools** â€” Connect papers, notes, and insights
- **Canonical Truth Stores** â€” Versioned facts, temporal validity, and append-only mutation history in a graph model
- **Financial Systems** â€” Loan/risk state reconstruction with as-of reads and audit receipts
- **Compliance & RegTech** â€” KYC/AML state changes, policy/rule versioning, and non-overlapping validity enforcement
- **Audit Platforms** â€” Correlate graph mutations to WAL sequence ranges and receipt hashes
- **AI Governance & Lineage** â€” Track model assertions, overrides, and fact provenance over time

## Documentation

| Guide                                                                      | Description                    |
| -------------------------------------------------------------------------- | ------------------------------ |
| [Getting Started](docs/getting-started/README.md)                          | Installation & quick start     |
| [Docker Image Quick Reference](docs/getting-started/image-quick-reference.md) | Full runtime image matrix   |
| [API Reference](docs/api-reference/README.md)                              | Cypher functions & procedures  |
| [User Guides](docs/user-guides/README.md)                                  | Complete examples & patterns   |
| [Performance](docs/performance/README.md)                                  | Benchmarks vs Neo4j            |
| [Neo4j Migration](docs/neo4j-migration/README.md)                          | Compatibility & feature parity |
| [Architecture](docs/architecture/README.md)                                | System design & internals      |
| [Docker Guide](docker/README.md)                                           | Build & deployment             |
| [Development](docs/development/README.md)                                  | Contributing & development     |

## Comparison

| Platform | Category | Query Language Support (and protocol) | Native Vector Search | Canonical Graph + Temporal Ledger Pattern | Queryable Mutation Log + Receipts | Embedded/Self-Hosted Focus |
| -------- | -------- | -------------------------------------- | -------------------- | ------------------------------------------ | ------------------------------- | -------------------------- |
| **NornicDB** | Graph + Vector + Canonical Ledger | **Cypher via Bolt**; also HTTP/GraphQL and gRPC (Qdrant-compatible + NornicSearch) | **Yes** | **Yes** | **Yes** | **Yes** |
| Neo4j | Graph DB | Cypher via Bolt/HTTP | Yes | Partial (manual modeling) | Partial (logs exist, not first-class receipts model) | Server-first |
| Memgraph | Graph DB | openCypher via Bolt/HTTP | Partial/varies by setup | Partial (manual) | Partial (manual/integration) | Server-first |
| TigerGraph | Graph analytics DB | GSQL via REST++/native endpoints | Partial/extension-driven | Partial (manual) | Partial (manual/integration) | Server-first |
| Qdrant | Vector DB | Qdrant query/filter API via gRPC/REST | Yes | No (not graph-native) | No | Server-first |
| Weaviate | Vector DB | GraphQL + REST APIs | Yes | Partial (knowledge graph features, not Cypher property graph) | No | Server-first |
| Amazon QLDB | Ledger DB | PartiQL via AWS API/SDK | No | Partial (ledger + temporal history, not graph-native) | Yes (ledger-native) | Managed service |

> Snapshot is capability-oriented and high-level; exact behavior depends on edition/configuration and workload design.

## Building

### Native Binary

```bash
# Basic build
make build

# Headless (no UI)
make build-headless

# With local LLM support
make build-localllm
```

### Docker Images

```bash
# Download models for Heimdall builds (automatic if missing)
make download-models        # BGE-M3 + qwen3-0.6b (~750MB)
make check-models          # Verify models present

# ARM64 (Apple Silicon)
make build-arm64-metal                  # Base (BYOM)
make build-arm64-metal-bge              # With BGE embeddings
make build-arm64-metal-bge-heimdall     # With BGE + Heimdall AI
make build-arm64-metal-headless         # Headless (no UI)

# AMD64 CUDA (NVIDIA GPU)
make build-amd64-cuda                   # Base (BYOM)
make build-amd64-cuda-bge               # With BGE embeddings
make build-amd64-cuda-bge-heimdall      # With BGE + Heimdall AI
make build-amd64-cuda-headless          # Headless (no UI)

# AMD64 CPU-only
make build-amd64-cpu                    # Minimal
make build-amd64-cpu-headless           # Minimal headless

# Build all variants for your architecture
make build-all

# Deploy to registry
make deploy-all             # Build + push all variants
```

### Cross-Compilation

```bash
# Build for other platforms from macOS
make cross-linux-amd64     # Linux x86_64
make cross-linux-arm64     # Linux ARM64
make cross-rpi             # Raspberry Pi 4/5
make cross-windows         # Windows (CPU-only)
make cross-all             # All platforms
```

## Roadmap

### Completed

- [x] Neo4j Bolt protocol
- [x] Cypher query engine (52 functions)
- [x] Memory decay system
- [x] GPU acceleration (Metal, CUDA)
- [x] Vector & full-text search
- [x] Auto-relationship engine
- [x] HNSW vector index
- [x] Metadata/Property Indexing
- [x] SIMD Implementation
- [x] Clustering support

### Planned (from `docs/plans`)

- [ ] Hybrid retrieval Phase 1: adaptive vector/BM25 execution order, cost-aware switching, and query telemetry (`docs/plans/scaling-search.md`)
- [ ] Distributed fabric Phase 1-2: QueryGateway, remote transport, shard routing, and distributed dispatch (`docs/plans/sharding.md`)
- [ ] Distributed transactions and vector search across shards (Fabric Phases 3-4) (`docs/plans/sharding.md`)
- [ ] Cluster admin APIs + UI/protocol integration for shard management (Fabric Phases 5-6) (`docs/plans/sharding.md`)
- [ ] GDPR compliance hardening: user-data detection, relationship export/delete/anonymization, and audit-log coverage (`docs/plans/gdpr-compliance-fixes.md`)

## Contributors

Special thanks to everyone who helps make NornicDB better. See [CONTRIBUTORS.md](CONTRIBUTORS.md) for a list of community contributors.

## License

MIT License â€” Originally part of the [Mimir](https://github.com/orneryd/mimir) project, now maintained as a standalone repository.

See [NOTICES.md](NOTICES.md) for third-party license information, including bundled AI models (BGE-M3, Qwen2.5) and dependencies.

---

<p align="center">
  <em>Weaving your data's destiny</em>
</p>
