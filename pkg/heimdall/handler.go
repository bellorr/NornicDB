package heimdall

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"regexp"
	"strings"
	"time"

	"github.com/orneryd/nornicdb/pkg/auth"
)

// Handler provides HTTP endpoints for Bifrost chat.
// Uses standard HTTP/SSE - no external dependencies required.
// Bifrost is the rainbow bridge that connects to Heimdall.
//
// Endpoints:
//   - GET  /api/bifrost/status           - Heimdall and Bifrost status
//   - POST /api/bifrost/chat/completions - Chat with Heimdall
//   - GET  /api/bifrost/events           - SSE stream for real-time events
type Handler struct {
	manager        *Manager
	bifrost        *Bifrost
	config         Config
	database       DatabaseRouter
	metrics        MetricsReader
	inMemoryRunner InMemoryToolRunner // optional: e.g. MCP store/recall/discover for agentic loop
}

// NewHandler creates a Bifrost HTTP handler.
// Returns nil if Heimdall is disabled (manager is nil).
// Automatically creates Bifrost bridge when Heimdall is enabled.
func NewHandler(manager *Manager, cfg Config, db DatabaseRouter, metrics MetricsReader) *Handler {
	if manager == nil {
		return nil
	}
	// Bifrost is automatically enabled when Heimdall is enabled
	bifrost := NewBifrost(cfg)
	return &Handler{
		manager:  manager,
		bifrost:  bifrost,
		config:   cfg,
		database: db,
		metrics:  metrics,
	}
}

// Bifrost returns the BifrostBridge for plugin communication.
// Returns NoOpBifrost if Bifrost is not available.
func (h *Handler) Bifrost() BifrostBridge {
	if h.bifrost == nil {
		return &NoOpBifrost{}
	}
	return h.bifrost
}

// SetInMemoryToolRunner sets the runner for MCP-style tools (store, recall, discover, etc.)
// so the agentic loop can call them in process. When set, tools from the runner are merged
// into the tool list and execution is dispatched in memory instead of via HTTP.
func (h *Handler) SetInMemoryToolRunner(runner InMemoryToolRunner) {
	h.inMemoryRunner = runner
}

// ServeHTTP routes requests to appropriate handlers.
func (h *Handler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	switch {
	case r.URL.Path == "/api/bifrost/status":
		h.handleStatus(w, r)
	case r.URL.Path == "/api/bifrost/chat/completions":
		h.handleChatCompletions(w, r)
	case r.URL.Path == "/api/bifrost/events":
		h.handleEvents(w, r)
	case r.URL.Path == "/api/bifrost/autocomplete":
		h.handleAutocomplete(w, r)
	default:
		http.NotFound(w, r)
	}
}

// handleStatus returns Heimdall status and stats.
// GET /api/bifrost/status
func (h *Handler) handleStatus(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodGet {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	stats := h.manager.Stats()

	// Include Bifrost stats if available
	var bifrostStats map[string]interface{}
	if h.bifrost != nil {
		bifrostStats = h.bifrost.Stats()
	} else {
		bifrostStats = map[string]interface{}{
			"enabled":          false,
			"connection_count": 0,
		}
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"status": "ok",
		"model":  h.config.Model,
		"heimdall": map[string]interface{}{
			"enabled": h.config.Enabled,
			"stats":   stats,
		},
		"bifrost": bifrostStats,
	})
}

// handleEvents provides an SSE stream for real-time Bifrost events.
// GET /api/bifrost/events
//
// This endpoint allows clients to receive real-time notifications, messages,
// and system events from Heimdall and its plugins.
func (h *Handler) handleEvents(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodGet {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	// Verify Bifrost is enabled
	if h.bifrost == nil {
		http.Error(w, "Bifrost not enabled", http.StatusServiceUnavailable)
		return
	}

	// Set SSE headers
	w.Header().Set("Content-Type", "text/event-stream")
	w.Header().Set("Cache-Control", "no-cache")
	w.Header().Set("Connection", "keep-alive")
	w.Header().Set("X-Accel-Buffering", "no") // Disable nginx buffering

	flusher, ok := w.(http.Flusher)
	if !ok {
		http.Error(w, "Streaming not supported", http.StatusInternalServerError)
		return
	}

	// Generate client ID
	clientID := generateID()

	// Register this connection with Bifrost
	h.bifrost.RegisterClient(clientID, w, flusher)
	defer h.bifrost.UnregisterClient(clientID)

	// Send initial connection message
	connMsg := BifrostMessage{
		Type:      "connected",
		Timestamp: time.Now().Unix(),
		Content:   "Connected to Bifrost",
		Data: map[string]interface{}{
			"client_id": clientID,
		},
	}
	data, _ := json.Marshal(connMsg)
	fmt.Fprintf(w, "data: %s\n\n", string(data))
	flusher.Flush()

	// Keep connection alive until client disconnects
	<-r.Context().Done()
}

// handleAutocomplete provides Cypher query autocomplete suggestions.
// POST /api/bifrost/autocomplete
//
// Request body:
//
//	{
//	  "query": "MATCH (n",
//	  "cursor_position": 10  // optional
//	}
//
// Response:
//
//	{
//	  "suggestion": "MATCH (n) RETURN n LIMIT 25",
//	  "schema": {
//	    "labels": ["Person", "File", ...],
//	    "properties": ["name", "age", ...],
//	    "relTypes": ["KNOWS", "OWNS", ...]
//	  }
//	}
func (h *Handler) handleAutocomplete(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req struct {
		Query          string `json:"query"`
		CursorPosition int    `json:"cursor_position,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	if req.Query == "" {
		http.Error(w, "query parameter required", http.StatusBadRequest)
		return
	}

	// Invoke the autocomplete action
	ctx := ActionContext{
		Context:            r.Context(),
		Params:             map[string]interface{}{"query": req.Query, "cursor_position": req.CursorPosition},
		Database:           h.database,
		Metrics:            h.metrics,
		Bifrost:            h.bifrost,
		PrincipalRoles:     PrincipalRolesFromContext(r.Context()),
		DatabaseAccessMode: DatabaseAccessModeFromContext(r.Context()),
		ResolvedAccess:     ResolvedAccessResolverFromContext(r.Context()),
	}

	result, err := ExecuteAction("heimdall_autocomplete_suggest", ctx)
	if err != nil {
		http.Error(w, fmt.Sprintf("Autocomplete error: %v", err), http.StatusInternalServerError)
		return
	}

	// If we have schema info but no suggestion, use SLM to generate one
	if result != nil && result.Success {
		schema, _ := result.Data["schema"].(map[string]interface{})
		suggestion, _ := result.Data["suggestion"].(string)

		// If no suggestion from action, use SLM to generate one
		if suggestion == "" && h.manager != nil {
			// Build prompt with schema context
			labels, _ := schema["labels"].([]interface{})
			properties, _ := schema["properties"].([]interface{})
			relTypes, _ := schema["relTypes"].([]interface{})

			labelStrs := make([]string, 0, len(labels))
			for _, l := range labels {
				if s, ok := l.(string); ok {
					labelStrs = append(labelStrs, s)
				}
			}
			propStrs := make([]string, 0, len(properties))
			for _, p := range properties {
				if s, ok := p.(string); ok {
					propStrs = append(propStrs, s)
				}
			}
			relStrs := make([]string, 0, len(relTypes))
			for _, r := range relTypes {
				if s, ok := r.(string); ok {
					relStrs = append(relStrs, s)
				}
			}

			// Build a clearer prompt that explains the difference between properties and relationship types
			var schemaContext strings.Builder
			if len(labelStrs) > 0 {
				schemaContext.WriteString(fmt.Sprintf("Node labels: %s\n", strings.Join(labelStrs, ", ")))
			}
			if len(propStrs) > 0 {
				// Limit properties to avoid overwhelming the prompt
				propsToShow := propStrs
				if len(propsToShow) > 20 {
					propsToShow = propsToShow[:20]
				}
				schemaContext.WriteString(fmt.Sprintf("Node properties (use as n.propertyName): %s\n", strings.Join(propsToShow, ", ")))
			}
			if len(relStrs) > 0 {
				// Limit relationship types
				relsToShow := relStrs
				if len(relsToShow) > 10 {
					relsToShow = relsToShow[:10]
				}
				schemaContext.WriteString(fmt.Sprintf("Relationship types (use in patterns like (a)-[:TYPE]->(b)): %s\n", strings.Join(relsToShow, ", ")))
			}

			prompt := fmt.Sprintf(`Complete this Cypher query. Output ONLY the single completed Cypher line. No explanations, no reasoning, no commentary about the user.

%s
Rules:
- Output ONLY the Cypher query line, nothing else
- No "the user", "they are", "I need to", or any meta-commentary
- Properties: n.propertyName. Relationships: (a)-[:TYPE]->(b)
- If missing LIMIT statement at the end, add LIMIT 25 at the end

Current query:
%s

Complete Cypher query (one line only):`,
				schemaContext.String(),
				req.Query)

			// Use raw prompt for direct completion with stricter parameters to prevent repetition
			slmResult, err := h.manager.Generate(r.Context(), prompt, GenerateParams{
				MaxTokens:   128, // Reduced to prevent repetition
				Temperature: 0.2, // Lower temperature for more focused output
				TopP:        0.8,
				TopK:        20,
				StopTokens:  []string{"The user", "They are", "I need to", "IMPORTANT", "Rules:", "Available", "Complete query:", "\n\n\n"},
			})
			if err == nil && slmResult != "" {
				// Clean up the suggestion - extract only the first valid Cypher query
				cleanSuggestion := strings.TrimSpace(slmResult)

				// Remove markdown code blocks
				cleanSuggestion = strings.TrimPrefix(cleanSuggestion, "```cypher")
				cleanSuggestion = strings.TrimPrefix(cleanSuggestion, "```")
				cleanSuggestion = strings.TrimSuffix(cleanSuggestion, "```")
				cleanSuggestion = strings.TrimSpace(cleanSuggestion)

				// Split by newlines and extract the first complete query
				lines := strings.Split(cleanSuggestion, "\n")
				cypherKeywordRegex := regexp.MustCompile(`^(MATCH|CREATE|MERGE|DELETE|SET|RETURN|WITH|UNWIND|CALL|LOAD|START|UNION|OPTIONAL)`)
				instructionPattern := regexp.MustCompile(`(?i)^(IMPORTANT|Rules?:|Available|Complete)`)

				var queryParts []string
				foundQuery := false

				for _, line := range lines {
					line = strings.TrimSpace(line)
					if line == "" {
						continue
					}

					// Stop immediately if we hit instruction patterns
					if instructionPattern.MatchString(line) {
						break
					}

					// If we found a query line, start collecting
					if cypherKeywordRegex.MatchString(line) {
						// If we already found a query and hit another one, stop (prevent repetition)
						if foundQuery {
							break
						}
						foundQuery = true
						queryParts = []string{line}
					} else if foundQuery {
						// Continue collecting query parts until we hit an instruction or another query
						if instructionPattern.MatchString(line) || cypherKeywordRegex.MatchString(line) {
							break
						}
						queryParts = append(queryParts, line)
					}
				}

				if len(queryParts) > 0 {
					cleanSuggestion = strings.Join(queryParts, " ")
					// Remove any remaining instruction text using regex
					cleanSuggestion = regexp.MustCompile(`(?i)\s+(IMPORTANT|Rules?:|Available|Complete).*$`).ReplaceAllString(cleanSuggestion, "")
					// Strip trailing model prose (e.g. "The user is a student..." or "They are asking for help...")
					proseStart := regexp.MustCompile(`(?i)\s+(The user|They are|I need to|I don't know|So they|Probably not|correct syntax|asking for help).*$`)
					cleanSuggestion = proseStart.ReplaceAllString(cleanSuggestion, "")
					cleanSuggestion = strings.TrimSpace(cleanSuggestion)

					// Remove any repetition patterns (query repeated multiple times)
					// Split by common delimiters and take the first unique query
					parts := regexp.MustCompile(`\s{2,}|\n{2,}`).Split(cleanSuggestion, -1)
					if len(parts) > 0 {
						cleanSuggestion = parts[0]
					}

					// Only use if it's a valid improvement and doesn't contain instruction text
					if cleanSuggestion != "" &&
						len(cleanSuggestion) > len(req.Query) &&
						cleanSuggestion != req.Query &&
						!strings.Contains(strings.ToUpper(cleanSuggestion), "IMPORTANT") &&
						!strings.Contains(strings.ToUpper(cleanSuggestion), "RULES") {
						suggestion = cleanSuggestion
					}
				}
			}
		}

		w.Header().Set("Content-Type", "application/json")
		json.NewEncoder(w).Encode(map[string]interface{}{
			"suggestion": suggestion,
			"schema":     schema,
		})
		return
	}

	// Fallback response
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"suggestion": "",
		"schema":     map[string]interface{}{},
	})
}

// sendCancellationResponse sends a cancellation response to the client.
// This is called when a lifecycle hook cancels the request.
func (h *Handler) sendCancellationResponse(w http.ResponseWriter, requestID, phase, cancelledBy, reason string) {
	// Log the cancellation
	log.Printf("[Bifrost] Request %s cancelled in %s by %s: %s", requestID, phase, cancelledBy, reason)

	// Send notification via Bifrost if available
	if h.bifrost != nil {
		h.bifrost.SendNotification("warning", "Request Cancelled",
			fmt.Sprintf("Request cancelled by %s: %s", cancelledBy, reason))
	}

	// Build cancellation response (OpenAI-compatible format)
	resp := ChatResponse{
		ID:      requestID,
		Object:  "chat.completion",
		Model:   h.config.Model,
		Created: time.Now().Unix(),
		Choices: []ChatChoice{
			{
				Index: 0,
				Message: &ChatMessage{
					Role: "assistant",
					Content: fmt.Sprintf("âš ï¸ Request cancelled by plugin\n\n"+
						"**Phase:** %s\n"+
						"**Cancelled by:** %s\n"+
						"**Reason:** %s",
						phase, cancelledBy, reason),
				},
				FinishReason: "stop",
			},
		},
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(resp)
}

// handleChatCompletions handles OpenAI-compatible chat completion requests via Bifrost.
// POST /api/bifrost/chat/completions
//
// Non-streaming returns JSON response.
// Streaming uses Server-Sent Events (SSE) - standard HTTP, no WebSocket needed.
//
// Request Lifecycle:
//  1. PrePrompt hook - plugins can modify prompt context
//  2. Build prompt with immutable ActionPrompt first
//  3. Send to Heimdall SLM
//  4. PreExecute hook - plugins can validate/modify before action runs
//  5. Execute action
//  6. PostExecute hook - plugins can log/update state
func (h *Handler) handleChatCompletions(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req ChatRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	// Default model if not specified (BYOM: only one model loaded)
	if req.Model == "" {
		req.Model = h.config.Model
	}

	// Extract user message for lifecycle context
	userMessage := ""
	for i := len(req.Messages) - 1; i >= 0; i-- {
		if req.Messages[i].Role == "user" {
			userMessage = req.Messages[i].Content
			break
		}
	}

	// Create PromptContext with immutable ActionPrompt
	requestID := generateID()
	promptCtx := &PromptContext{
		RequestID:    requestID,
		RequestTime:  time.Now(),
		ActionPrompt: ActionPrompt(), // IMMUTABLE - always first
		UserMessage:  userMessage,
		Messages:     req.Messages,
		Examples:     defaultExamples(),
		PluginData:   make(map[string]interface{}),
	}
	// Set Bifrost for notifications (fire-and-forget SSE messages)
	promptCtx.SetBifrost(h.bifrost)

	// === Phase 1: PrePrompt hooks (optional) ===
	// Plugins that implement PrePromptHook can modify the prompt context
	// Plugins can call promptCtx.Cancel() to abort the request
	CallPrePromptHooks(promptCtx)
	if promptCtx.Cancelled() {
		log.Printf("[Bifrost] Request cancelled by %s: %s", promptCtx.CancelledBy(), promptCtx.CancelReason())
		h.sendCancellationResponse(w, promptCtx.RequestID, "PrePrompt", promptCtx.CancelledBy(), promptCtx.CancelReason())
		return
	}

	// === Phase 2: Build final prompt ===
	// ActionPrompt is always at the start (immutable)
	systemContent := promptCtx.BuildFinalPrompt()
	systemMsg := ChatMessage{Role: "system", Content: systemContent}

	// Validate token budget before proceeding
	if err := promptCtx.ValidateTokenBudget(); err != nil {
		budgetInfo := promptCtx.GetBudgetInfo()
		log.Printf("[Bifrost] Token budget exceeded: %v (system: %d, user: %d, total: %d)",
			err, budgetInfo.SystemTokens, budgetInfo.UserTokens, budgetInfo.TotalTokens)
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}

	// Build messages: system + user message
	messages := []ChatMessage{systemMsg}
	for _, msg := range promptCtx.Messages {
		if msg.Role != "system" { // Skip original system messages
			messages = append(messages, msg)
		}
	}

	prompt := BuildPrompt(messages)

	// Generation params
	params := GenerateParams{
		MaxTokens:   req.MaxTokens,
		Temperature: req.Temperature,
		TopP:        req.TopP,
		TopK:        20, // Qwen3 0.6B instruct best practice to reduce repetition
		StopTokens:  []string{"<|im_end|>", "<|endoftext|>", "</s>"},
	}
	if params.MaxTokens == 0 {
		params.MaxTokens = h.config.MaxTokens
	}
	if params.Temperature == 0 {
		params.Temperature = h.config.Temperature
	}

	ctx, cancel := context.WithTimeout(r.Context(), 60*time.Second)
	defer cancel()

	// Store PromptContext in request context for later phases; attach RBAC from request context
	lifecycleCtx := &requestLifecycle{
		promptCtx:              promptCtx,
		requestID:              requestID,
		database:               h.database,
		metrics:                h.metrics,
		principalRoles:         PrincipalRolesFromContext(r.Context()),
		databaseAccessMode:     DatabaseAccessModeFromContext(r.Context()),
		resolvedAccessResolver: ResolvedAccessResolverFromContext(r.Context()),
	}

	if req.Stream {
		if h.manager.SupportsTools() {
			h.handleStreamingWithTools(w, ctx, prompt, params, req.Model, lifecycleCtx)
		} else {
			h.handleStreamingResponse(w, ctx, prompt, params, req.Model, lifecycleCtx)
		}
	} else {
		h.handleNonStreamingResponse(w, ctx, prompt, params, req.Model, lifecycleCtx)
	}
}

// requestLifecycle holds state through the request lifecycle for hooks.
// When StreamWriter is set (streaming + tools path), the agentic loop sends
// PreExecute/PostExecute notifications as SSE chunks and the handler streams the final response.
type requestLifecycle struct {
	promptCtx     *PromptContext
	requestID     string
	database      DatabaseRouter
	metrics       MetricsReader
	StreamWriter  http.ResponseWriter // optional: for streaming notifications during agentic loop
	StreamFlusher http.Flusher        // optional: flush after each SSE chunk
	StreamModel   string              // optional: model name for SSE chunk payloads
	// RBAC: from request context (set by server when Bifrost is behind auth)
	principalRoles         []string
	databaseAccessMode     auth.DatabaseAccessMode
	resolvedAccessResolver func(string) auth.ResolvedAccess
}

// sendStreamNotifications writes queued notifications as SSE chunks (used when streaming with tools).
func (h *Handler) sendStreamNotifications(lifecycle *requestLifecycle, notifs []QueuedNotification) {
	if lifecycle.StreamWriter == nil || lifecycle.StreamFlusher == nil || lifecycle.StreamModel == "" {
		return
	}
	for _, notif := range notifs {
		icon := "â„¹ï¸"
		switch notif.Type {
		case "error":
			icon = "âŒ"
		case "warning":
			icon = "âš ï¸"
		case "success":
			icon = "âœ…"
		case "progress":
			icon = "ðŸ”„"
		}
		chunk := ChatResponse{
			ID:      lifecycle.requestID,
			Object:  "chat.completion.chunk",
			Model:   lifecycle.StreamModel,
			Created: time.Now().Unix(),
			Choices: []ChatChoice{
				{
					Index: 0,
					Delta: &ChatMessage{
						Role:    "heimdall",
						Content: fmt.Sprintf("[Heimdall]: %s %s: %s\n", icon, notif.Title, notif.Message),
					},
				},
			},
		}
		data, _ := json.Marshal(chunk)
		fmt.Fprintf(lifecycle.StreamWriter, "data: %s\n\n", data)
		lifecycle.StreamFlusher.Flush()
	}
}

// defaultExamples returns built-in examples for action mapping.
// These help Heimdall understand common user intents and map them to actions.
func defaultExamples() []PromptExample {
	return []PromptExample{
		// === STATUS & METRICS ===
		{UserSays: "status", ActionJSON: `{"action": "heimdall_watcher_status", "params": {}}`},
		{UserSays: "what is the status", ActionJSON: `{"action": "heimdall_watcher_status", "params": {}}`},
		{UserSays: "show me metrics", ActionJSON: `{"action": "heimdall_watcher_metrics", "params": {}}`},
		{UserSays: "database stats", ActionJSON: `{"action": "heimdall_watcher_db_stats", "params": {}}`},
		{UserSays: "health check", ActionJSON: `{"action": "heimdall_watcher_status", "params": {}}`},

		// === CLUSTERING & EMBEDDINGS (db_stats) ===
		{UserSays: "how many k-means clusters", ActionJSON: `{"action": "heimdall_watcher_db_stats", "params": {}}`},
		{UserSays: "show clustering info", ActionJSON: `{"action": "heimdall_watcher_db_stats", "params": {}}`},
		{UserSays: "how many embeddings", ActionJSON: `{"action": "heimdall_watcher_db_stats", "params": {}}`},
		{UserSays: "what features are enabled", ActionJSON: `{"action": "heimdall_watcher_db_stats", "params": {}}`},
		{UserSays: "show feature flags", ActionJSON: `{"action": "heimdall_watcher_db_stats", "params": {}}`},

		// === COUNTING & STATISTICS ===
		{UserSays: "how many nodes", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n) RETURN count(n) AS total_nodes"}}`},
		{UserSays: "count all relationships", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH ()-[r]->() RETURN count(r) AS total_relationships"}}`},
		{UserSays: "what labels exist", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "CALL db.labels() YIELD label RETURN label"}}`},
		{UserSays: "show relationship types", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "CALL db.relationshipTypes() YIELD relationshipType RETURN relationshipType"}}`},

		// === SAMPLING & EXPLORATION ===
		{UserSays: "show me some nodes", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n) RETURN n LIMIT 10"}}`},
		{UserSays: "sample Person nodes", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n:Person) RETURN n LIMIT 5"}}`},
		{UserSays: "cypher query for nodes with label Animal", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n:Animal) RETURN n"}}`},
		{UserSays: "nodes with label :Animal", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n:Animal) RETURN n"}}`},
		{UserSays: "show relationships", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (a)-[r]->(b) RETURN a, type(r), b LIMIT 10"}}`},

		// === SEARCHING ===
		{UserSays: "find nodes with name Alice", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n {name: 'Alice'}) RETURN n"}}`},
		{UserSays: "search for nodes containing test", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n) WHERE n.name CONTAINS 'test' RETURN n LIMIT 20"}}`},

		// === AGGREGATIONS ===
		{UserSays: "nodes per label", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n) RETURN labels(n) AS label, count(n) AS count ORDER BY count DESC"}}`},
		{UserSays: "relationship distribution", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH ()-[r]->() RETURN type(r) AS type, count(r) AS count ORDER BY count DESC"}}`},

		// === GRAPH ANALYSIS ===
		{UserSays: "find highly connected nodes", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n)-[r]-() RETURN n, count(r) AS connections ORDER BY connections DESC LIMIT 10"}}`},
		{UserSays: "orphan nodes", ActionJSON: `{"action": "heimdall_watcher_query", "params": {"cypher": "MATCH (n) WHERE NOT (n)--() RETURN n LIMIT 20"}}`},

		// === SEMANTIC SEARCH (discover) ===
		// Generic examples - domain-specific examples come from domain plugins
		{UserSays: "search for X", ActionJSON: `{"action": "heimdall_watcher_discover", "params": {"query": "X"}}`},
		{UserSays: "find information about Y", ActionJSON: `{"action": "heimdall_watcher_discover", "params": {"query": "Y"}}`},
		{UserSays: "what do we know about Z", ActionJSON: `{"action": "heimdall_watcher_discover", "params": {"query": "Z"}}`},
		{UserSays: "tell me about topic", ActionJSON: `{"action": "heimdall_watcher_discover", "params": {"query": "topic"}}`},
	}
}

// runAgenticLoop runs the agentic loop for any provider: execute actions, feed results back, repeat until final answer.
// For tool-capable providers (OpenAI/Ollama) uses GenerateWithTools; for local GGUF uses prompt-based multi-round.
// When inMemoryRunner is set (e.g. MCP server), its tools (store, recall, discover, etc.) are included so the LLM can manage memories in process.
func (h *Handler) runAgenticLoop(ctx context.Context, lifecycle *requestLifecycle, systemPrompt, userMessage string, params GenerateParams) (finalResponse string, err error) {
	tools := ActionsAsMCPTools()
	if h.inMemoryRunner != nil {
		tools = append(tools, h.inMemoryRunner.ToolDefinitions()...)
	}
	if h.manager.SupportsTools() && len(tools) > 0 {
		return h.runAgenticLoopWithTools(ctx, lifecycle, userMessage, tools, params)
	}
	return h.runAgenticLoopPromptBased(ctx, lifecycle, systemPrompt, userMessage, params)
}

// runAgenticLoopWithTools uses native tool calling (OpenAI/Ollama). Execute toolCalls, append results, repeat.
func (h *Handler) runAgenticLoopWithTools(ctx context.Context, lifecycle *requestLifecycle, userMessage string, tools []MCPTool, params GenerateParams) (string, error) {
	messages := []ToolRoundMessage{
		{Role: "system", Content: AgenticSystemPromptTools},
		{Role: "user", Content: userMessage},
	}
	var lastContent string
	for round := 0; round < MaxAgenticRounds; round++ {
		content, toolCalls, err := h.manager.GenerateWithTools(ctx, messages, tools, params)
		if err != nil {
			return "", err
		}
		lastContent = content
		if len(toolCalls) == 0 {
			if lastContent != "" {
				return lastContent, nil
			}
			continue
		}
		// Append assistant message with tool_calls
		assistantMsg := ToolRoundMessage{Role: "assistant", Content: content, ToolCalls: toolCalls}
		for i := range assistantMsg.ToolCalls {
			if assistantMsg.ToolCalls[i].Id == "" {
				assistantMsg.ToolCalls[i].Id = "call_" + generateID()
			}
		}
		messages = append(messages, assistantMsg)
		// Execute each tool and append tool result messages
		for _, tc := range toolCalls {
			var paramsMap map[string]interface{}
			if tc.Arguments != "" {
				_ = json.Unmarshal([]byte(tc.Arguments), &paramsMap)
			}
			if paramsMap == nil {
				paramsMap = make(map[string]interface{})
			}
			preExecCtx := &PreExecuteContext{
				RequestID: lifecycle.requestID, RequestTime: lifecycle.promptCtx.RequestTime,
				Action: tc.Name, Params: paramsMap, PluginData: lifecycle.promptCtx.PluginData,
				Database: lifecycle.database, Metrics: lifecycle.metrics,
				PrincipalRoles:     lifecycle.principalRoles,
				DatabaseAccessMode: lifecycle.databaseAccessMode,
				ResolvedAccess:     lifecycle.resolvedAccessResolver,
			}
			preExecCtx.SetBifrost(h.bifrost)
			preExecResult := CallPreExecuteHooks(preExecCtx)
			h.sendStreamNotifications(lifecycle, preExecCtx.DrainNotifications())
			if preExecCtx.Cancelled() {
				messages = append(messages, ToolRoundMessage{Role: "tool", ToolCallID: tc.Id, Content: "Cancelled: " + preExecCtx.CancelReason()})
				continue
			}
			if !preExecResult.Continue {
				messages = append(messages, ToolRoundMessage{Role: "tool", ToolCallID: tc.Id, Content: preExecResult.AbortMessage})
				continue
			}
			if preExecResult.ModifiedParams != nil {
				paramsMap = preExecResult.ModifiedParams
			}
			startTime := time.Now()
			var toolContent string
			if h.inMemoryRunner != nil && sliceContains(h.inMemoryRunner.ToolNames(), tc.Name) {
				dbName := lifecycle.database.DefaultDatabaseName()
				raw, execErr := h.inMemoryRunner.CallTool(ctx, tc.Name, paramsMap, dbName)
				toolContent = FormatInMemoryToolResult(raw, execErr)
				execDuration := time.Since(startTime)
				hookResult := &ActionResult{Success: execErr == nil}
				if execErr != nil {
					hookResult.Message = execErr.Error()
				} else {
					hookResult.Message = "tool completed successfully"
					hookResult.Data = map[string]interface{}{"result": raw}
				}
				postExecCtx := &PostExecuteContext{RequestID: lifecycle.requestID, Action: tc.Name, Params: paramsMap, Result: hookResult, Duration: execDuration, PluginData: lifecycle.promptCtx.PluginData}
				CallPostExecuteHooks(postExecCtx)
				h.sendStreamNotifications(lifecycle, postExecCtx.DrainNotifications())
			} else {
				actCtx := ActionContext{
					Context: ctx, UserMessage: userMessage, Params: paramsMap,
					Bifrost: h.bifrost, Database: lifecycle.database, Metrics: lifecycle.metrics,
					PrincipalRoles:     lifecycle.principalRoles,
					DatabaseAccessMode: lifecycle.databaseAccessMode,
					ResolvedAccess:     lifecycle.resolvedAccessResolver,
				}
				result, execErr := ExecuteAction(tc.Name, actCtx)
				execDuration := time.Since(startTime)
				if execErr != nil {
					toolContent = FormatActionResultForModel(&ActionResult{Success: false, Message: execErr.Error()})
				} else {
					postExecCtx := &PostExecuteContext{RequestID: lifecycle.requestID, Action: tc.Name, Params: paramsMap, Result: result, Duration: execDuration, PluginData: lifecycle.promptCtx.PluginData}
					CallPostExecuteHooks(postExecCtx)
					h.sendStreamNotifications(lifecycle, postExecCtx.DrainNotifications())
					toolContent = FormatActionResultForModel(result)
				}
			}
			messages = append(messages, ToolRoundMessage{Role: "tool", ToolCallID: tc.Id, Content: toolContent})
		}
	}
	if lastContent != "" {
		return lastContent, nil
	}
	return "I've completed the available actions. Is there anything else?", nil
}

func sliceContains(ss []string, s string) bool {
	for _, x := range ss {
		if x == s {
			return true
		}
	}
	return false
}

// runAgenticLoopPromptBased uses prompt-based multi-round for local GGUF (or any provider without native tools).
func (h *Handler) runAgenticLoopPromptBased(ctx context.Context, lifecycle *requestLifecycle, systemPrompt, userMessage string, params GenerateParams) (string, error) {
	prompt := systemPrompt + "\n\nUser: " + userMessage + "\n\nRespond with either {\"action\": \"...\", \"params\": {...}} or a direct answer.\n\nAssistant: "
	var lastResponse string
	for round := 0; round < MaxAgenticRounds; round++ {
		response, err := h.manager.Generate(ctx, prompt, params)
		if err != nil {
			return "", err
		}
		response = strings.TrimSpace(response)
		lastResponse = response
		parsedAction, actionError := h.tryParseAction(response)
		if actionError != "" {
			return actionError, nil
		}
		if parsedAction == nil {
			return response, nil
		}
		preExecCtx := &PreExecuteContext{
			RequestID: lifecycle.requestID, RequestTime: lifecycle.promptCtx.RequestTime,
			Action: parsedAction.Action, Params: parsedAction.Params, RawResponse: response, PluginData: lifecycle.promptCtx.PluginData,
			Database: lifecycle.database, Metrics: lifecycle.metrics,
			PrincipalRoles:     lifecycle.principalRoles,
			DatabaseAccessMode: lifecycle.databaseAccessMode,
			ResolvedAccess:     lifecycle.resolvedAccessResolver,
		}
		preExecCtx.SetBifrost(h.bifrost)
		preExecResult := CallPreExecuteHooks(preExecCtx)
		if preExecCtx.Cancelled() {
			return preExecCtx.CancelReason(), nil
		}
		if !preExecResult.Continue {
			if preExecResult.AbortMessage != "" {
				return preExecResult.AbortMessage, nil
			}
			return "Action aborted.", nil
		}
		paramsToUse := parsedAction.Params
		if preExecResult.ModifiedParams != nil {
			paramsToUse = preExecResult.ModifiedParams
		}
		startTime := time.Now()
		var resultStr string
		if h.inMemoryRunner != nil && sliceContains(h.inMemoryRunner.ToolNames(), parsedAction.Action) {
			dbName := lifecycle.database.DefaultDatabaseName()
			raw, execErr := h.inMemoryRunner.CallTool(ctx, parsedAction.Action, paramsToUse, dbName)
			resultStr = FormatInMemoryToolResult(raw, execErr)
			execDuration := time.Since(startTime)
			hookResult := &ActionResult{Success: execErr == nil}
			if execErr != nil {
				hookResult.Message = execErr.Error()
			} else {
				hookResult.Message = "tool completed successfully"
				hookResult.Data = map[string]interface{}{"result": raw}
			}
			CallPostExecuteHooks(&PostExecuteContext{RequestID: lifecycle.requestID, Action: parsedAction.Action, Params: paramsToUse, Result: hookResult, Duration: execDuration, PluginData: lifecycle.promptCtx.PluginData})
		} else {
			actCtx := ActionContext{
				Context: ctx, UserMessage: userMessage, Params: paramsToUse,
				Bifrost: h.bifrost, Database: lifecycle.database, Metrics: lifecycle.metrics,
				PrincipalRoles:     lifecycle.principalRoles,
				DatabaseAccessMode: lifecycle.databaseAccessMode,
				ResolvedAccess:     lifecycle.resolvedAccessResolver,
			}
			result, execErr := ExecuteAction(parsedAction.Action, actCtx)
			execDuration := time.Since(startTime)
			if execErr != nil {
				prompt = prompt + response + "\n\nTool result: " + FormatActionResultForModel(&ActionResult{Success: false, Message: execErr.Error()}) + "\n\nBased on the above, respond with another action or a final answer.\n\nAssistant: "
				continue
			}
			CallPostExecuteHooks(&PostExecuteContext{RequestID: lifecycle.requestID, Action: parsedAction.Action, Params: paramsToUse, Result: result, Duration: execDuration, PluginData: lifecycle.promptCtx.PluginData})
			resultStr = FormatActionResultForModel(result)
		}
		prompt = prompt + response + "\n\nTool result: " + resultStr + "\n\nBased on the tool result, respond with another action JSON or a final answer to the user.\n\nAssistant: "
	}
	return lastResponse, nil
}

// handleNonStreamingResponse generates complete response with lifecycle hooks.
// Uses agentic loop for any provider: execute actions, feed results back, LLM infers and formats final response.
func (h *Handler) handleNonStreamingResponse(w http.ResponseWriter, ctx context.Context, prompt string, params GenerateParams, model string, lifecycle *requestLifecycle) {
	finalResponse, err := h.runAgenticLoop(ctx, lifecycle, lifecycle.promptCtx.BuildFinalPrompt(), lifecycle.promptCtx.UserMessage, params)
	if err != nil {
		http.Error(w, fmt.Sprintf("Generation error: %v", err), http.StatusInternalServerError)
		return
	}

	log.Printf("[Bifrost] Agentic loop finished: %s", finalResponse)

	resp := ChatResponse{
		ID:      lifecycle.requestID,
		Object:  "chat.completion", // OpenAI API compatible
		Model:   model,
		Created: time.Now().Unix(),
		Choices: []ChatChoice{
			{
				Index: 0,
				Message: &ChatMessage{
					Role:    "assistant",
					Content: finalResponse,
				},
				FinishReason: "stop",
			},
		},
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(resp)
}

// handleStreamingWithTools runs the agentic loop (tools path) and streams the final response.
// Used when stream=true and the provider supports tools (OpenAI/Ollama). The LLM sees tool
// results and produces a final answer (e.g. "Here's what a node looks like: ..."); we stream
// that answer and send PreExecute/PostExecute notifications during the loop.
func (h *Handler) handleStreamingWithTools(w http.ResponseWriter, ctx context.Context, prompt string, params GenerateParams, model string, lifecycle *requestLifecycle) {
	w.Header().Set("Content-Type", "text/event-stream")
	w.Header().Set("Cache-Control", "no-cache")
	w.Header().Set("Connection", "keep-alive")
	w.Header().Set("X-Accel-Buffering", "no")

	flusher, ok := w.(http.Flusher)
	if !ok {
		http.Error(w, "Streaming not supported", http.StatusInternalServerError)
		return
	}

	// Set stream writer so PrePrompt and agentic-loop notifications use the same path
	lifecycle.StreamWriter = w
	lifecycle.StreamFlusher = flusher
	lifecycle.StreamModel = model
	h.sendStreamNotifications(lifecycle, lifecycle.promptCtx.DrainNotifications())

	finalResponse, err := h.runAgenticLoop(ctx, lifecycle, lifecycle.promptCtx.BuildFinalPrompt(), lifecycle.promptCtx.UserMessage, params)
	if err != nil {
		chunk := ChatResponse{
			ID: lifecycle.requestID, Object: "chat.completion.chunk", Model: model, Created: time.Now().Unix(),
			Choices: []ChatChoice{{
				Index: 0,
				Delta: &ChatMessage{Content: fmt.Sprintf("Error: %v", err)},
			}},
		}
		data, _ := json.Marshal(chunk)
		fmt.Fprintf(w, "data: %s\n\n", data)
		flusher.Flush()
	} else if finalResponse != "" {
		// Stream the final assistant response as one content chunk (OpenAI format)
		chunk := ChatResponse{
			ID:      lifecycle.requestID,
			Object:  "chat.completion.chunk",
			Model:   model,
			Created: time.Now().Unix(),
			Choices: []ChatChoice{{
				Index: 0,
				Delta: &ChatMessage{Content: finalResponse},
			}},
		}
		data, _ := json.Marshal(chunk)
		fmt.Fprintf(w, "data: %s\n\n", data)
		flusher.Flush()
	}

	// Send finish chunk and [DONE]
	doneChunk := ChatResponse{
		ID: lifecycle.requestID, Object: "chat.completion.chunk", Model: model, Created: time.Now().Unix(),
		Choices: []ChatChoice{{Index: 0, Delta: &ChatMessage{}, FinishReason: "stop"}},
	}
	data, _ := json.Marshal(doneChunk)
	fmt.Fprintf(w, "data: %s\n\n", data)
	fmt.Fprintf(w, "data: [DONE]\n\n")
	flusher.Flush()
}

// tryParseAction parses action JSON from SLM response.
// Format: {"action": "heimdall_watcher_status", "params": {}}
// Returns (parsedAction, errorMessage). If errorMessage is set, action is invalid.
func (h *Handler) tryParseAction(response string) (*ParsedAction, string) {
	response = strings.TrimSpace(response)

	// Find JSON in response
	start := strings.Index(response, "{")
	if start == -1 {
		log.Printf("[Bifrost] tryParseAction: no JSON start found")
		return nil, ""
	}
	end := strings.LastIndex(response, "}")
	if end == -1 || end <= start {
		log.Printf("[Bifrost] tryParseAction: no JSON end found")
		return nil, ""
	}

	jsonStr := response[start : end+1]
	log.Printf("[Bifrost] tryParseAction: parsing JSON: %s", jsonStr)

	var parsed ParsedAction
	if err := json.Unmarshal([]byte(jsonStr), &parsed); err != nil {
		log.Printf("[Bifrost] tryParseAction: JSON parse error: %v", err)
		return nil, ""
	}

	if parsed.Action == "" {
		log.Printf("[Bifrost] tryParseAction: no action field")
		return nil, ""
	}

	log.Printf("[Bifrost] tryParseAction: looking up action: %s", parsed.Action)
	actions := ListHeimdallActions()
	log.Printf("[Bifrost] tryParseAction: registered actions: %v", actions)

	if _, ok := GetHeimdallAction(parsed.Action); !ok {
		log.Printf("[Bifrost] tryParseAction: action NOT FOUND: %s", parsed.Action)
		return nil, fmt.Sprintf("Sorry, I don't know how to perform the action '%s'. Try asking 'what can you do?' to see available actions.", parsed.Action)
	}

	log.Printf("[Bifrost] tryParseAction: action FOUND: %s", parsed.Action)
	return &parsed, ""
}

// handleStreamingResponse uses Server-Sent Events (SSE) for streaming with lifecycle hooks.
// SSE is standard HTTP - works with any HTTP client, no WebSocket needed.
// After streaming completes, checks for action commands and executes them.
func (h *Handler) handleStreamingResponse(w http.ResponseWriter, ctx context.Context, prompt string, params GenerateParams, model string, lifecycle *requestLifecycle) {
	// Set SSE headers
	w.Header().Set("Content-Type", "text/event-stream")
	w.Header().Set("Cache-Control", "no-cache")
	w.Header().Set("Connection", "keep-alive")
	w.Header().Set("X-Accel-Buffering", "no") // Disable nginx buffering

	flusher, ok := w.(http.Flusher)
	if !ok {
		http.Error(w, "Streaming not supported", http.StatusInternalServerError)
		return
	}

	id := lifecycle.requestID

	// === Send queued notifications from PrePrompt hooks inline ===
	// This ensures proper ordering - notifications appear before the AI response
	notifications := lifecycle.promptCtx.DrainNotifications()
	for _, notif := range notifications {
		icon := "â„¹ï¸"
		switch notif.Type {
		case "error":
			icon = "âŒ"
		case "warning":
			icon = "âš ï¸"
		case "success":
			icon = "âœ…"
		case "progress":
			icon = "ðŸ”„"
		}

		notifChunk := ChatResponse{
			ID:      id,
			Object:  "chat.completion.chunk",
			Model:   model,
			Created: time.Now().Unix(),
			Choices: []ChatChoice{
				{
					Index: 0,
					Delta: &ChatMessage{
						Role:    "heimdall",
						Content: fmt.Sprintf("[Heimdall]: %s %s: %s\n", icon, notif.Title, notif.Message),
					},
				},
			},
		}
		data, _ := json.Marshal(notifChunk)
		fmt.Fprintf(w, "data: %s\n\n", data)
		flusher.Flush()
	}

	// Collect full response to check for actions
	var fullResponse strings.Builder
	isActionResponse := false

	// Stream tokens - but buffer action JSON responses instead of streaming
	err := h.manager.GenerateStream(ctx, prompt, params, func(token string) error {
		fullResponse.WriteString(token)

		// Detect if this looks like an action JSON response
		// If response starts with '{', buffer it instead of streaming
		currentResponse := strings.TrimSpace(fullResponse.String())
		if strings.HasPrefix(currentResponse, "{") {
			isActionResponse = true
		}

		// Don't stream action JSON to user - we'll send the synthesized result instead
		if isActionResponse {
			return nil
		}

		chunk := ChatResponse{
			ID:      id,
			Object:  "chat.completion.chunk", // OpenAI API streaming format
			Model:   model,
			Created: time.Now().Unix(),
			Choices: []ChatChoice{
				{
					Index: 0,
					Delta: &ChatMessage{
						Content: token,
					},
				},
			},
		}

		data, _ := json.Marshal(chunk)
		fmt.Fprintf(w, "data: %s\n\n", data)
		flusher.Flush()
		return nil
	})

	if err != nil {
		// Send error event
		fmt.Fprintf(w, "data: {\"error\": \"%s\"}\n\n", err.Error())
		flusher.Flush()
		return
	}

	// Check if response contains an action command
	response := fullResponse.String()
	log.Printf("[Bifrost] Streaming complete, checking for action: %s", response)

	parsedAction, actionError := h.tryParseAction(response)

	// Handle action not found error
	if actionError != "" {
		fmt.Fprintf(w, "data: %s\n\n", actionError)
		flusher.Flush()
		fmt.Fprintf(w, "data: [DONE]\n\n")
		flusher.Flush()
		return
	}

	if parsedAction != nil {
		log.Printf("[Bifrost] Action detected in stream: %s", parsedAction.Action)

		// === Phase 4: PreExecute hooks ===
		preExecCtx := &PreExecuteContext{
			RequestID:          lifecycle.requestID,
			RequestTime:        lifecycle.promptCtx.RequestTime,
			Action:             parsedAction.Action,
			Params:             parsedAction.Params,
			RawResponse:        response,
			PluginData:         lifecycle.promptCtx.PluginData,
			Database:           lifecycle.database,
			Metrics:            lifecycle.metrics,
			PrincipalRoles:     lifecycle.principalRoles,
			DatabaseAccessMode: lifecycle.databaseAccessMode,
			ResolvedAccess:     lifecycle.resolvedAccessResolver,
		}
		// Set Bifrost for notifications (fire-and-forget SSE messages)
		preExecCtx.SetBifrost(h.bifrost)

		// === PreExecute hooks (optional) ===
		// Plugins that implement PreExecuteHook can validate/modify params
		preExecResult := CallPreExecuteHooks(preExecCtx)
		cancelled := preExecCtx.Cancelled()
		if cancelled {
			log.Printf("[Bifrost] Request cancelled by %s: %s", preExecCtx.CancelledBy(), preExecCtx.CancelReason())
			// Send cancellation as SSE chunk
			cancelChunk := ChatResponse{
				ID:      id,
				Object:  "chat.completion.chunk",
				Model:   model,
				Created: time.Now().Unix(),
				Choices: []ChatChoice{
					{
						Index: 0,
						Delta: &ChatMessage{
							Content: fmt.Sprintf("\n\nâš ï¸ Request cancelled by %s: %s",
								preExecCtx.CancelledBy(), preExecCtx.CancelReason()),
						},
					},
				},
			}
			data, _ := json.Marshal(cancelChunk)
			fmt.Fprintf(w, "data: %s\n\n", data)
			flusher.Flush()
		}

		if !cancelled {
			// === Send PreExecute notifications inline ===
			preExecNotifications := preExecCtx.DrainNotifications()
			for _, notif := range preExecNotifications {
				icon := "â„¹ï¸"
				switch notif.Type {
				case "error":
					icon = "âŒ"
				case "warning":
					icon = "âš ï¸"
				case "success":
					icon = "âœ…"
				case "progress":
					icon = "ðŸ”„"
				}
				notifChunk := ChatResponse{
					ID:      id,
					Object:  "chat.completion.chunk",
					Model:   model,
					Created: time.Now().Unix(),
					Choices: []ChatChoice{
						{
							Index: 0,
							Delta: &ChatMessage{
								Role:    "heimdall",
								Content: fmt.Sprintf("[Heimdall]: %s %s: %s\n", icon, notif.Title, notif.Message),
							},
						},
					},
				}
				data, _ := json.Marshal(notifChunk)
				fmt.Fprintf(w, "data: %s\n\n", data)
				flusher.Flush()
			}

			var actionResponse string
			var result *ActionResult
			var execDuration time.Duration

			if !preExecResult.Continue {
				actionResponse = preExecResult.AbortMessage
				if actionResponse == "" {
					actionResponse = "Action aborted by plugin"
				}
			} else {
				// === Phase 5: Execute action ===
				startTime := time.Now()
				var err error
				if h.inMemoryRunner != nil && sliceContains(h.inMemoryRunner.ToolNames(), parsedAction.Action) {
					dbName := lifecycle.database.DefaultDatabaseName()
					raw, callErr := h.inMemoryRunner.CallTool(ctx, parsedAction.Action, parsedAction.Params, dbName)
					err = callErr
					execDuration = time.Since(startTime)
					if err != nil {
						actionResponse = fmt.Sprintf("Action failed: %v", err)
						result = &ActionResult{Success: false, Message: actionResponse}
					} else {
						actionResponse = "Action completed successfully"
						result = &ActionResult{Success: true, Message: actionResponse, Data: map[string]interface{}{"result": raw}}
					}
				} else {
					actCtx := ActionContext{
						Context:            ctx,
						UserMessage:        prompt,
						Params:             parsedAction.Params,
						Bifrost:            h.bifrost,
						Database:           h.database,
						Metrics:            h.metrics,
						PrincipalRoles:     lifecycle.principalRoles,
						DatabaseAccessMode: lifecycle.databaseAccessMode,
						ResolvedAccess:     lifecycle.resolvedAccessResolver,
					}
					result, err = ExecuteAction(parsedAction.Action, actCtx)
					execDuration = time.Since(startTime)
				}

				if err != nil {
					log.Printf("[Bifrost] Action execution failed: %v", err)
					actionResponse = fmt.Sprintf("Action failed: %v", err)
				} else if result != nil {
					log.Printf("[Bifrost] Action result: success=%v", result.Success)
				}

				// === Phase 6: PostExecute hooks (optional) ===
				// Plugins that implement PostExecuteHook get notified
				postExecCtx := &PostExecuteContext{
					RequestID:  lifecycle.requestID,
					Action:     parsedAction.Action,
					Params:     parsedAction.Params,
					Result:     result,
					Duration:   execDuration,
					PluginData: lifecycle.promptCtx.PluginData,
				}
				CallPostExecuteHooks(postExecCtx)

				// === Send PostExecute notifications inline ===
				postExecNotifications := postExecCtx.DrainNotifications()
				for _, notif := range postExecNotifications {
					icon := "â„¹ï¸"
					switch notif.Type {
					case "error":
						icon = "âŒ"
					case "warning":
						icon = "âš ï¸"
					case "success":
						icon = "âœ…"
					case "progress":
						icon = "ðŸ”„"
					}
					notifChunk := ChatResponse{
						ID:      id,
						Object:  "chat.completion.chunk",
						Model:   model,
						Created: time.Now().Unix(),
						Choices: []ChatChoice{
							{
								Index: 0,
								Delta: &ChatMessage{
									Role:    "heimdall",
									Content: fmt.Sprintf("[Heimdall]: %s %s: %s\n", icon, notif.Title, notif.Message),
								},
							},
						},
					}
					data, _ := json.Marshal(notifChunk)
					fmt.Fprintf(w, "data: %s\n\n", data)
					flusher.Flush()
				}

				// === Phase 7: Synthesis hooks ===
				// Allow plugins to provide custom response formatting
				if result != nil {
					synthCtx := &SynthesisContext{
						RequestID:    lifecycle.requestID,
						UserQuestion: prompt,
						Action:       parsedAction.Action,
						Result:       result,
						PluginData:   lifecycle.promptCtx.PluginData,
						Database:     h.database,
					}

					// First, try plugin synthesis hooks
					if pluginResponse := CallSynthesisHooks(synthCtx); pluginResponse != "" {
						log.Printf("[Bifrost] Using plugin-provided synthesis")
						actionResponse = "\n\n" + pluginResponse
					} else {
						// Fall back to simple formatting (no LLM synthesis)
						log.Printf("[Bifrost] Using default format (no plugin synthesis)")
						actionResponse = "\n\n" + h.defaultFormatResponse(result)
					}
				}
			}

			// Send action result chunk
			resultChunk := ChatResponse{
				ID:      id,
				Object:  "chat.completion.chunk",
				Model:   model,
				Created: time.Now().Unix(),
				Choices: []ChatChoice{
					{
						Index: 0,
						Delta: &ChatMessage{
							Content: actionResponse,
						},
					},
				},
			}
			data, _ := json.Marshal(resultChunk)
			fmt.Fprintf(w, "data: %s\n\n", data)
			flusher.Flush()
		}
	}

	// Send final chunk with finish_reason (OpenAI format)
	doneChunk := ChatResponse{
		ID:      id,
		Object:  "chat.completion.chunk", // OpenAI API streaming format
		Model:   model,
		Created: time.Now().Unix(),
		Choices: []ChatChoice{
			{
				Index:        0,
				Delta:        &ChatMessage{},
				FinishReason: "stop",
			},
		},
	}
	data, _ := json.Marshal(doneChunk)
	fmt.Fprintf(w, "data: %s\n\n", data)
	// OpenAI sends [DONE] to signal stream end
	fmt.Fprintf(w, "data: [DONE]\n\n")
	flusher.Flush()
}

// defaultFormatResponse provides a simple fallback format for action results.
// This is a no-op that just formats the data - actual synthesis should be done by plugins.
//
// Plugins that implement SynthesisHook can provide rich, domain-specific formatting
// (e.g., the watcher plugin provides LLM-based prose synthesis).
func (h *Handler) defaultFormatResponse(result *ActionResult) string {
	if result == nil {
		return "No results available."
	}

	if !result.Success {
		return "Action failed: " + result.Message
	}

	// If there's no structured data, just return the message
	if len(result.Data) == 0 {
		return result.Message
	}

	// Simple format: message + JSON data
	dataJSON, err := json.MarshalIndent(result.Data, "", "  ")
	if err != nil {
		return result.Message
	}

	return result.Message + "\n\n```json\n" + string(dataJSON) + "\n```"
}
