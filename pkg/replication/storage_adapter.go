// Package replication provides distributed replication for NornicDB.
package replication

import (
	"context"
	"fmt"
	"io"
	"log"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/orneryd/nornicdb/pkg/cypher"
	"github.com/orneryd/nornicdb/pkg/storage"
)

// StorageAdapter bridges the replication.Storage interface to storage.Engine.
// It translates replication commands into storage operations and maintains WAL state.
type StorageAdapter struct {
	engine   storage.Engine
	executor *cypher.StorageExecutor // Cypher executor for executing replicated Cypher queries

	// Persistent WAL for replication commands
	wal         *storage.WAL
	walDir      string
	walMu       sync.RWMutex // Protects wal and walPosition
	walPosition atomic.Uint64

	// In-memory WAL for fast streaming (avoids re-reading wal.log continuously).
	memWALMu sync.RWMutex
	memWAL   []*WALEntry // sorted by Position asc

	// Async WAL writing for performance
	walQueue   chan *walWriteRequest
	walBatch   []*walWriteRequest
	walBatchMu sync.Mutex
	walStopCh  chan struct{}
	walWg      sync.WaitGroup
}

// walWriteRequest represents a pending WAL write.
type walWriteRequest struct {
	record  replicationWALRecord
	posCh   chan uint64 // Channel to receive the WAL position
	errCh   chan error  // Channel to receive any error
	waiting bool        // If true, caller is waiting for completion
}

type replicationWALRecord struct {
	// Timestamp when the entry was created.
	Timestamp int64 `json:"ts"`

	// Command is the replicated command.
	Command *Command `json:"cmd"`
}

// NewStorageAdapter creates a new storage adapter wrapping the given engine.
// The WAL directory defaults to "data/replication/wal" if not specified.
func NewStorageAdapter(engine storage.Engine) (*StorageAdapter, error) {
	return NewStorageAdapterWithWAL(engine, "")
}

// NewStorageAdapterWithWAL creates a new storage adapter with a custom WAL directory.
// If walDir is empty, defaults to "data/replication/wal".
func NewStorageAdapterWithWAL(engine storage.Engine, walDir string) (*StorageAdapter, error) {
	if walDir == "" {
		walDir = "data/replication/wal"
	}

	// Create WAL directory if needed
	if err := os.MkdirAll(walDir, 0755); err != nil {
		return nil, fmt.Errorf("failed to create WAL directory: %w", err)
	}

	// Create persistent WAL
	walConfig := storage.DefaultWALConfig()
	walConfig.Dir = walDir
	walConfig.SyncMode = "batch" // Batch sync for performance
	walConfig.BatchSyncInterval = 100 * time.Millisecond

	wal, err := storage.NewWAL(walDir, walConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create WAL: %w", err)
	}

	adapter := &StorageAdapter{
		engine:    engine,
		executor:  cypher.NewStorageExecutor(engine),
		wal:       wal,
		walDir:    walDir,
		walQueue:  make(chan *walWriteRequest, 1000), // Buffered channel for async WAL writes
		walBatch:  make([]*walWriteRequest, 0, 100),  // Pre-allocated batch buffer
		walStopCh: make(chan struct{}),
	}

	// Load existing WAL position
	_ = adapter.loadWALPosition()

	// Start async WAL writer
	adapter.walWg.Add(1)
	go adapter.walWriterLoop()

	return adapter, nil
}

// loadWALPosition loads the last WAL position from persistent storage.
func (a *StorageAdapter) loadWALPosition() error {
	// storage.WAL already recovers its own sequence on startup.
	// Use that as the authoritative replication WAL position.
	if a.wal != nil {
		a.walPosition.Store(a.wal.Sequence())
	}
	return nil
}

// SetExecutor sets a custom Cypher executor for the adapter.
// This allows using an executor with additional configuration (e.g., database manager, embedder).
func (a *StorageAdapter) SetExecutor(executor *cypher.StorageExecutor) {
	a.executor = executor
}

// ApplyCommand applies a replicated command to storage.
// WAL writes are now asynchronous for better performance.
func (a *StorageAdapter) ApplyCommand(cmd *Command) error {
	if cmd == nil {
		return fmt.Errorf("nil command")
	}

	// Record in persistent WAL (write-ahead logging)
	record := replicationWALRecord{
		Timestamp: cmd.Timestamp.UnixNano(),
		Command:   cmd,
	}

	// Queue async WAL write (non-blocking)
	req := &walWriteRequest{
		record:  record,
		posCh:   make(chan uint64, 1),
		errCh:   make(chan error, 1),
		waiting: false, // Don't wait for WAL write to complete
	}

	// Try to queue async write
	select {
	case a.walQueue <- req:
		// Successfully queued, continue
	default:
		// Channel full - fall back to synchronous write to avoid blocking
		// This should be rare with a 1000-item buffer
		a.walMu.Lock()
		if err := a.wal.Append(storage.OperationType("replication_command"), record); err != nil {
			a.walMu.Unlock()
			return fmt.Errorf("failed to append to WAL: %w", err)
		}
		pos := a.wal.Sequence()
		a.walPosition.Store(pos)
		a.walMu.Unlock()
		a.appendToMemWAL(pos, record, cmd)
	}

	// Execute the command immediately (don't wait for WAL write)
	// This allows storage operations to proceed in parallel with WAL I/O
	var execErr error
	switch cmd.Type {
	case CmdCreateNode:
		execErr = a.applyCreateNode(cmd.Data)
	case CmdUpdateNode:
		execErr = a.applyUpdateNode(cmd.Data)
	case CmdDeleteNode:
		execErr = a.applyDeleteNode(cmd.Data)
	case CmdCreateEdge:
		execErr = a.applyCreateEdge(cmd.Data)
	case CmdUpdateEdge:
		execErr = a.applyUpdateEdge(cmd.Data)
	case CmdDeleteEdge:
		execErr = a.applyDeleteEdge(cmd.Data)
	case CmdSetProperty:
		execErr = a.applySetProperty(cmd.Data)
	case CmdBatchWrite:
		execErr = a.applyBatchWrite(cmd.Data)
	case CmdCypher:
		execErr = a.applyCypher(cmd.Data)
	case CmdDeleteByPrefix:
		execErr = a.applyDeleteByPrefix(cmd.Data)
	case CmdBulkCreateNodes:
		execErr = a.applyBulkCreateNodes(cmd.Data)
	case CmdBulkCreateEdges:
		execErr = a.applyBulkCreateEdges(cmd.Data)
	case CmdBulkDeleteNodes:
		execErr = a.applyBulkDeleteNodes(cmd.Data)
	case CmdBulkDeleteEdges:
		execErr = a.applyBulkDeleteEdges(cmd.Data)
	default:
		execErr = fmt.Errorf("unknown command type: %d", cmd.Type)
	}

	// Try to get position from async write (non-blocking, best-effort)
	// If not ready yet, walWriterLoop will update it later
	select {
	case pos := <-req.posCh:
		a.walPosition.Store(pos)
		a.appendToMemWAL(pos, record, cmd)
	case err := <-req.errCh:
		if err != nil {
			// Log error but don't fail the operation (WAL is for durability, not correctness)
			// The command was already applied to storage
			log.Printf("[WAL] Async write error (non-fatal): %v", err)
		}
	default:
		// Position not ready yet, will be set by walWriterLoop
		// This is fine - the command was applied and WAL write is queued
	}

	return execErr
}

// appendToMemWAL appends an entry to the in-memory WAL for fast streaming.
func (a *StorageAdapter) appendToMemWAL(pos uint64, record replicationWALRecord, cmd *Command) {
	a.memWALMu.Lock()
	a.memWAL = append(a.memWAL, &WALEntry{
		Position:  pos,
		Timestamp: record.Timestamp,
		Command:   cmd,
	})
	a.memWALMu.Unlock()
}

// walWriterLoop processes WAL writes asynchronously in batches.
func (a *StorageAdapter) walWriterLoop() {
	defer a.walWg.Done()

	ticker := time.NewTicker(10 * time.Millisecond) // Batch interval
	defer ticker.Stop()

	for {
		select {
		case <-a.walStopCh:
			// Flush remaining writes
			a.flushWALBatch()
			return
		case req := <-a.walQueue:
			// Add to batch
			a.walBatchMu.Lock()
			a.walBatch = append(a.walBatch, req)
			batchSize := len(a.walBatch)
			a.walBatchMu.Unlock()

			// Flush if batch is large enough (100 items)
			if batchSize >= 100 {
				a.flushWALBatch()
			}
		case <-ticker.C:
			// Periodic flush
			a.flushWALBatch()
		}
	}
}

// flushWALBatch writes all pending WAL entries.
// Uses individual appends but processes them in a batch to reduce lock contention.
func (a *StorageAdapter) flushWALBatch() {
	a.walBatchMu.Lock()
	if len(a.walBatch) == 0 {
		a.walBatchMu.Unlock()
		return
	}
	batch := a.walBatch
	a.walBatch = a.walBatch[:0] // Clear batch
	a.walBatchMu.Unlock()

	// Write each entry to WAL in a single lock to reduce contention
	// The WAL's internal buffering will batch the actual file writes
	a.walMu.Lock()
	positions := make([]uint64, 0, len(batch))
	for _, req := range batch {
		if err := a.wal.Append(storage.OperationType("replication_command"), req.record); err != nil {
			// Send error to waiting request
			select {
			case req.errCh <- fmt.Errorf("failed to append to WAL: %w", err):
			default:
			}
			positions = append(positions, 0) // Placeholder for failed entry
			continue
		}
		// Get position after append (sequence is incremented atomically by WAL)
		pos := a.wal.Sequence()
		positions = append(positions, pos)

		// Send position to request (non-blocking)
		select {
		case req.posCh <- pos:
		default:
		}
	}
	// Update global position
	if len(batch) > 0 {
		a.walPosition.Store(a.wal.Sequence())
	}
	a.walMu.Unlock()

	// Update in-memory WAL outside the lock to avoid deadlock
	for i, req := range batch {
		if positions[i] > 0 {
			a.appendToMemWAL(positions[i], req.record, req.record.Command)
		}
	}
}

// applyCreateNode creates a node from command data.
func (a *StorageAdapter) applyCreateNode(data []byte) error {
	node, err := decodeNodePayload(data)
	if err != nil {
		return fmt.Errorf("decode node: %w", err)
	}
	_, err = a.engine.CreateNode(node)
	return err
}

// applyUpdateNode updates a node from command data.
func (a *StorageAdapter) applyUpdateNode(data []byte) error {
	node, err := decodeNodePayload(data)
	if err != nil {
		return fmt.Errorf("decode node: %w", err)
	}
	return a.engine.UpdateNode(node)
}

// applyDeleteNode deletes a node.
func (a *StorageAdapter) applyDeleteNode(data []byte) error {
	var req struct {
		NodeID string
	}
	if err := decodeGob(data, &req); err == nil && req.NodeID != "" {
		return a.engine.DeleteNode(storage.NodeID(req.NodeID))
	}
	// Legacy fallback: raw ID bytes.
	return a.engine.DeleteNode(storage.NodeID(string(data)))
}

// applyCreateEdge creates an edge from command data.
func (a *StorageAdapter) applyCreateEdge(data []byte) error {
	edge, err := decodeEdgePayload(data)
	if err != nil {
		return fmt.Errorf("decode edge: %w", err)
	}
	return a.engine.CreateEdge(edge)
}

func (a *StorageAdapter) applyUpdateEdge(data []byte) error {
	edge, err := decodeEdgePayload(data)
	if err != nil {
		return fmt.Errorf("decode edge: %w", err)
	}
	return a.engine.UpdateEdge(edge)
}

// applyDeleteEdge deletes an edge.
func (a *StorageAdapter) applyDeleteEdge(data []byte) error {
	var req struct {
		EdgeID string
	}
	if err := decodeGob(data, &req); err != nil {
		return fmt.Errorf("decode delete edge request: %w", err)
	}
	return a.engine.DeleteEdge(storage.EdgeID(req.EdgeID))
}

// applySetProperty sets a property on a node.
func (a *StorageAdapter) applySetProperty(data []byte) error {
	var req struct {
		NodeID string
		Key    string
		Value  interface{}
	}
	if err := decodeGob(data, &req); err != nil {
		return fmt.Errorf("decode set property request: %w", err)
	}

	// Get node, update property, save
	node, err := a.engine.GetNode(storage.NodeID(req.NodeID))
	if err != nil {
		return err
	}
	if node.Properties == nil {
		node.Properties = make(map[string]interface{})
	}
	node.Properties[req.Key] = req.Value
	return a.engine.UpdateNode(node)
}

// applyBatchWrite applies a batch of operations.
func (a *StorageAdapter) applyBatchWrite(data []byte) error {
	var batch struct {
		Nodes [][]byte
		Edges [][]byte
	}
	if err := decodeGob(data, &batch); err != nil {
		return fmt.Errorf("decode batch: %w", err)
	}

	for _, nodeBytes := range batch.Nodes {
		node, err := decodeNodePayload(nodeBytes)
		if err != nil {
			return err
		}
		if _, err := a.engine.CreateNode(node); err != nil {
			return err
		}
	}
	for _, edgeBytes := range batch.Edges {
		edge, err := decodeEdgePayload(edgeBytes)
		if err != nil {
			return err
		}
		if err := a.engine.CreateEdge(edge); err != nil {
			return err
		}
	}
	return nil
}

func (a *StorageAdapter) applyDeleteByPrefix(data []byte) error {
	var req struct {
		Prefix string
	}
	if err := decodeGob(data, &req); err != nil {
		return fmt.Errorf("decode delete by prefix request: %w", err)
	}
	if req.Prefix == "" {
		return fmt.Errorf("prefix is required")
	}
	_, _, err := a.engine.DeleteByPrefix(req.Prefix)
	return err
}

func (a *StorageAdapter) applyBulkCreateNodes(data []byte) error {
	var encoded [][]byte
	if err := decodeGob(data, &encoded); err != nil {
		return fmt.Errorf("decode bulk create nodes: %w", err)
	}
	nodes := make([]*storage.Node, 0, len(encoded))
	for _, b := range encoded {
		n, err := decodeNodePayload(b)
		if err != nil {
			return err
		}
		nodes = append(nodes, n)
	}
	return a.engine.BulkCreateNodes(nodes)
}

func (a *StorageAdapter) applyBulkCreateEdges(data []byte) error {
	var encoded [][]byte
	if err := decodeGob(data, &encoded); err != nil {
		return fmt.Errorf("decode bulk create edges: %w", err)
	}
	edges := make([]*storage.Edge, 0, len(encoded))
	for _, b := range encoded {
		edge, err := decodeEdgePayload(b)
		if err != nil {
			return err
		}
		edges = append(edges, edge)
	}
	return a.engine.BulkCreateEdges(edges)
}

func (a *StorageAdapter) applyBulkDeleteNodes(data []byte) error {
	var ids []storage.NodeID
	if err := decodeGob(data, &ids); err != nil {
		return fmt.Errorf("decode bulk delete nodes: %w", err)
	}
	return a.engine.BulkDeleteNodes(ids)
}

func (a *StorageAdapter) applyBulkDeleteEdges(data []byte) error {
	var ids []storage.EdgeID
	if err := decodeGob(data, &ids); err != nil {
		return fmt.Errorf("decode bulk delete edges: %w", err)
	}
	return a.engine.BulkDeleteEdges(ids)
}

// applyCypher executes a Cypher command (for write queries).
// The data should be a JSON object with "query" (string) and optional "params" (map[string]interface{}).
func (a *StorageAdapter) applyCypher(data []byte) error {
	if a.executor == nil {
		return fmt.Errorf("cypher executor not available - cannot execute Cypher command")
	}

	// Parse Cypher command data
	var cypherCmd struct {
		Query  string
		Params map[string]interface{}
	}

	if err := decodeGob(data, &cypherCmd); err != nil {
		return fmt.Errorf("unmarshal cypher command: %w", err)
	}

	if cypherCmd.Query == "" {
		return fmt.Errorf("cypher query is empty")
	}

	// Execute the Cypher query
	// Use background context since this is a replicated command (no user context)
	ctx := context.Background()
	_, err := a.executor.Execute(ctx, cypherCmd.Query, cypherCmd.Params)
	if err != nil {
		return fmt.Errorf("execute cypher query: %w", err)
	}

	return nil
}

// FlushWAL waits for all pending WAL writes to complete.
// This is useful for tests or when you need to ensure durability before proceeding.
func (a *StorageAdapter) FlushWAL() error {
	// Flush any pending batch
	a.flushWALBatch()

	// Wait a bit for async writes to complete
	time.Sleep(20 * time.Millisecond)

	// Sync the WAL to ensure all writes are on disk
	a.walMu.Lock()
	defer a.walMu.Unlock()
	if a.wal != nil {
		return a.wal.Sync()
	}
	return nil
}

// Close releases replication resources (WAL file handles/background goroutines).
func (a *StorageAdapter) Close() error {
	// Stop async WAL writer (only once)
	select {
	case <-a.walStopCh:
		// Already closed
	default:
		close(a.walStopCh)
	}
	a.walWg.Wait()

	a.walMu.Lock()
	defer a.walMu.Unlock()
	if a.wal != nil {
		err := a.wal.Close()
		a.wal = nil
		return err
	}
	return nil
}

// GetWALPosition returns the current WAL position.
func (a *StorageAdapter) GetWALPosition() (uint64, error) {
	return a.walPosition.Load(), nil
}

// GetWALEntries returns WAL entries starting from the given position.
func (a *StorageAdapter) GetWALEntries(fromPosition uint64, maxEntries int) ([]*WALEntry, error) {
	// Fast path: serve from in-memory WAL.
	a.memWALMu.RLock()
	mem := a.memWAL
	a.memWALMu.RUnlock()

	if len(mem) > 0 && fromPosition >= mem[0].Position {
		// Binary search first entry with Position > fromPosition
		lo, hi := 0, len(mem)
		for lo < hi {
			mid := (lo + hi) / 2
			if mem[mid].Position <= fromPosition {
				lo = mid + 1
			} else {
				hi = mid
			}
		}
		if lo >= len(mem) {
			return []*WALEntry{}, nil
		}
		end := lo + maxEntries
		if end > len(mem) {
			end = len(mem)
		}
		entries := make([]*WALEntry, end-lo)
		copy(entries, mem[lo:end])
		return entries, nil
	}

	// Read from persistent WAL
	a.walMu.RLock()
	defer a.walMu.RUnlock()

	walPath := filepath.Join(a.walDir, "wal.log")
	storageEntries, err := storage.ReadWALEntries(walPath)
	if err != nil {
		// Handle missing WAL file gracefully (may not exist yet)
		if os.IsNotExist(err) {
			return []*WALEntry{}, nil
		}
		// Check if error message indicates file not found (storage.ReadWALEntries may wrap the error)
		errStr := err.Error()
		if strings.Contains(errStr, "no such file") || strings.Contains(errStr, "not found") {
			return []*WALEntry{}, nil
		}
		return nil, fmt.Errorf("failed to read WAL entries: %w", err)
	}

	var entries []*WALEntry
	for _, storageEntry := range storageEntries {
		// Only process replication_command entries
		if storageEntry.Operation != storage.OperationType("replication_command") {
			continue
		}

		// Prefer the current record format, but tolerate older WALs.
		var (
			ts  int64
			cmd *Command
		)
		var rec replicationWALRecord
		if err := decodeGob(storageEntry.Data, &rec); err == nil && rec.Command != nil {
			ts = rec.Timestamp
			cmd = rec.Command
		} else {
			continue // Skip corrupted/legacy entries
		}

		pos := storageEntry.Sequence
		if pos > fromPosition {
			entries = append(entries, &WALEntry{
				Position:  pos,
				Timestamp: ts,
				Command:   cmd,
			})
			if len(entries) >= maxEntries {
				break
			}
		}
	}

	return entries, nil
}

// PruneWALEntries drops in-memory WAL entries up to (and including) uptoPosition.
// This keeps memory bounded while streaming and does not affect the persistent WAL.
func (a *StorageAdapter) PruneWALEntries(uptoPosition uint64) {
	a.memWALMu.Lock()
	defer a.memWALMu.Unlock()
	if len(a.memWAL) == 0 {
		return
	}
	// Find first entry with Position > uptoPosition.
	lo, hi := 0, len(a.memWAL)
	for lo < hi {
		mid := (lo + hi) / 2
		if a.memWAL[mid].Position <= uptoPosition {
			lo = mid + 1
		} else {
			hi = mid
		}
	}
	if lo == 0 {
		return
	}
	// Drop prefix in-place to avoid allocating a new backing array.
	copy(a.memWAL, a.memWAL[lo:])
	a.memWAL = a.memWAL[:len(a.memWAL)-lo]
}

// WriteSnapshot writes a full snapshot to the given writer.
func (a *StorageAdapter) WriteSnapshot(w SnapshotWriter) error {
	// Get all nodes and edges
	nodes, err := a.engine.AllNodes()
	if err != nil {
		return fmt.Errorf("get all nodes: %w", err)
	}

	edges, err := a.engine.AllEdges()
	if err != nil {
		return fmt.Errorf("get all edges: %w", err)
	}

	snapshot := struct {
		WALPosition uint64
		Nodes       []*storage.Node
		Edges       []*storage.Edge
	}{
		WALPosition: a.walPosition.Load(),
		Nodes:       nodes,
		Edges:       edges,
	}

	data, err := encodeGob(snapshot)
	if err != nil {
		return fmt.Errorf("encode snapshot: %w", err)
	}

	_, err = w.Write(data)
	return err
}

// RestoreSnapshot restores state from a snapshot.
func (a *StorageAdapter) RestoreSnapshot(r SnapshotReader) error {
	data, err := io.ReadAll(r)
	if err != nil {
		return fmt.Errorf("read snapshot: %w", err)
	}

	var snapshot struct {
		WALPosition uint64
		Nodes       []*storage.Node
		Edges       []*storage.Edge
	}

	if err := decodeGob(data, &snapshot); err != nil {
		return fmt.Errorf("decode snapshot: %w", err)
	}

	// Restore nodes
	for _, node := range snapshot.Nodes {
		if _, err := a.engine.CreateNode(node); err != nil {
			return fmt.Errorf("restore node: %w", err)
		}
	}

	// Restore edges
	for _, edge := range snapshot.Edges {
		if err := a.engine.CreateEdge(edge); err != nil {
			return fmt.Errorf("restore edge: %w", err)
		}
	}

	// Restore WAL position
	a.walPosition.Store(snapshot.WALPosition)

	return nil
}

// Engine returns the underlying storage engine.
func (a *StorageAdapter) Engine() storage.Engine {
	return a.engine
}

// Verify StorageAdapter implements Storage interface.
var _ Storage = (*StorageAdapter)(nil)
