# Heimdall Model Training Guide

**Heimdall** is NornicDB's specialized Small Language Model (SLM) optimized for:
- Cypher query generation and understanding
- NornicDB database management operations
- Heimdall plugin system expertise
- Graph database optimization and troubleshooting

## Training Approach: Fine-Tuning, Not From Scratch

**We use Parameter-Efficient Fine-Tuning (PEFT) with LoRA**, not training from scratch:

âœ… **What we do:** Fine-tune existing models (TinyLlama, Qwen, Phi-3)
- Trains only 1-2% of model weights (~20M parameters out of 1.5B)
- Base model provides language understanding
- We specialize it for Cypher/NornicDB operations
- **Training time:** 6-8 hours on consumer GPU
- **Data needed:** 1K-10K examples

âŒ **What we DON'T do:** Train from scratch
- Would require training all ~1.5B parameters
- Needs millions of examples and weeks of training
- Requires expensive GPU clusters
- Results are actually worse without massive datasets

**Why LoRA is better:**
1. **Faster:** Hours vs weeks
2. **Cheaper:** Works on 2080 Ti or M1 Mac
3. **Better results:** Pre-trained models already understand language
4. **Less data:** 5K examples vs 1B+ tokens
5. **Memory efficient:** 4-8GB VRAM vs 80GB+

---

## Table of Contents

- [Quick Start](#quick-start)
- [Context Size Optimization](#context-size-optimization)
- [Parameter Size Recommendations](#parameter-size-recommendations)
- [Training Configuration](#training-configuration)
- [Dataset Generation](#dataset-generation)
- [Training Process](#training-process)
- [Evaluation & Validation](#evaluation--validation)
- [Deployment](#deployment)

---

## Quick Start

### 1. Generate Heimdall Dataset

```bash
cd neural/

# Generate comprehensive training data
python scripts/generate_heimdall_dataset.py \
    --repo /Users/timothysweet/src/NornicDB \
    --output data/heimdall_training.jsonl \
    --num-examples 5000
```

**Dataset composition:**
- 30% - Cypher fundamentals (MATCH, CREATE, WHERE, aggregations)
- 40% - NornicDB operations (indexing, vector search, transactions)
- 20% - Heimdall plugin system (APOC procedures, custom functions)
- 10% - Troubleshooting and optimization

### 2. Validate & Correct Dataset

**The generator prevents duplicates but may have category imbalance. Always run the corrector:**

```bash
# Step 1: Validate raw dataset
python scripts/validate_dataset.py data/heimdall_training.jsonl

# Step 2: Auto-correct (edits in-place, creates timestamped backup)
python scripts/correct_dataset.py data/heimdall_training.jsonl \
    --min-category-size 50 \
    --max-category-ratio 3.0

# Step 3: Validate corrected dataset
python scripts/validate_dataset.py data/heimdall_training.jsonl
```

**Note:** The corrector automatically creates a timestamped backup (e.g., `heimdall_training_backup_20251206_143022.jsonl`) before editing in-place.

**What the corrector does:**
- **Removes duplicates** - Ensures unique training examples (MD5 hash based)
- **Balances categories** - Downsamples over-represented categories, upsamples under-represented
- **Validates format** - Ensures all examples have required fields
- **Min category size** - Ensures each category has â‰¥50 examples (adjustable)
- **Max ratio** - Limits imbalance to 3:1 between largest/smallest categories

**Expected improvement:**
- **Before:** 154:1 imbalance, ~750 examples
- **After:** 3:1 imbalance, ~900+ examples balanced
- **Backup:** Automatically saved with timestamp

### 3. Choose Your Heimdall Variant

**Two options available:**

| Variant | License | Base Model | Params | Final Size (Q4) | Training Time | Quality |
|---------|---------|------------|--------|-----------------|---------------|----------|
| **heimdall** | Apache 2.0 | Qwen2.5-1.5B | 1.5B | **~1.0 GB** | 6-8 hours | Excellent |
| **heimdall_mit** | MIT | Phi-3-mini | 3.8B | **~2.2 GB** | 10-12 hours | Superior |

**Choose `heimdall` (Apache 2.0) if:**
- âœ… You want faster training (6-8 hours)
- âœ… You want smaller model size (~1 GB)
- âœ… Apache 2.0 license is sufficient
- âœ… You need excellent multilingual support
- âœ… You have limited VRAM (8GB works)

**Choose `heimdall_mit` (MIT) if:**
- âœ… You require MIT license (no restrictions)
- âœ… You want highest quality output
- âœ… You can afford 2.2 GB model size
- âœ… You have 11GB+ VRAM for training
- âœ… Better reasoning is worth extra training time

### 4. Train Heimdall Model (LoRA Fine-Tuning)

#### Option A: Apache 2.0 Variant (Recommended)

```bash
# For NVIDIA GPU (2080 Ti with 11GB VRAM)
python train.py \
    --preset heimdall \
    --dataset data/heimdall_training.jsonl \
    --output_dir models/heimdall \
    --epochs 10

# For Apple Silicon (M1/M2/M3)
python train.py \
    --preset heimdall \
    --dataset data/heimdall_training.jsonl \
    --output_dir models/heimdall \
    --epochs 10 \
    --use_metal
```

**Training stats:**
- Base model: Qwen2.5-1.5B-Instruct (Apache 2.0)
- Trainable params: ~31M (2.1% of 1.5B)
- VRAM usage: 8-9 GB
- Training time: 6-8 hours (2080 Ti) or 8-10 hours (M1/M2)
- Final model size: ~1.0 GB (Q4 quantized)

#### Option B: MIT Licensed Variant (Higher Quality)

```bash
# For NVIDIA GPU (2080 Ti with 11GB VRAM) - requires QLoRA
python train.py \
    --preset heimdall_mit \
    --dataset data/heimdall_training.jsonl \
    --output_dir models/heimdall_mit \
    --epochs 10

# For Apple Silicon with 16GB+ unified memory
python train.py \
    --preset heimdall_mit \
    --dataset data/heimdall_training.jsonl \
    --output_dir models/heimdall_mit \
    --epochs 10 \
    --use_metal
```

**Training stats:**
- Base model: Phi-3-mini-4k-instruct (MIT)
- Trainable params: ~31M (0.8% of 3.8B)
- VRAM usage: 10-11 GB (with QLoRA)
- Training time: 10-12 hours (2080 Ti) or 12-15 hours (M1/M2)
- Final model size: ~2.2 GB (Q4 quantized)

**What happens during training:**
1. **Downloads base model** - Qwen2.5-1.5B (~3GB) from HuggingFace
2. **Initializes LoRA adapters** - Creates ~20M trainable parameters
3. **Freezes base model** - Only LoRA weights are trained
4. **Fine-tunes on Cypher** - Learns NornicDB-specific patterns
5. **Saves checkpoints** - Best model saved based on validation loss

### 5. Export to GGUF

**For Apache 2.0 variant:**
```bash
python export_to_gguf.py \
    --model_dir models/heimdall \
    --output heimdall-q4.gguf \
    --quant_type Q4_K_M
```

**For MIT variant:**
```bash
python export_to_gguf.py \
    --model_dir models/heimdall_mit \
    --output heimdall-mit-q4.gguf \
    --quant_type Q4_K_M
```

**Quantization options:**
- `Q4_K_M`: Best balance (~1.0 GB / ~2.2 GB) - **Recommended**
- `Q5_K_M`: Better quality (~1.3 GB / ~2.7 GB)
- `Q8_0`: Highest quality (~1.8 GB / ~4.0 GB)

### 6. Deploy to NornicDB

**For Apache 2.0 variant:**
```bash
# Copy to Heimdall plugin directory
mkdir -p ../plugins/heimdall/models
cp heimdall-q4.gguf ../plugins/heimdall/models/

# Update config
# Edit nornicdb.yaml:
# heimdall:
#   model_path: plugins/heimdall/models/heimdall-q4.gguf
#   context_size: 2048
#   license: "Apache 2.0"
```

**For MIT variant:**
```bash
# Copy to Heimdall plugin directory
mkdir -p ../plugins/heimdall/models
cp heimdall-mit-q4.gguf ../plugins/heimdall/models/

# Update config
# Edit nornicdb.yaml:
# heimdall:
#   model_path: plugins/heimdall/models/heimdall-mit-q4.gguf
#   context_size: 2048
#   license: "MIT"
```

---

## Context Size Optimization

Context size directly impacts memory usage and inference speed. Choose based on your use case.

### Context Size Recommendations

| Context Size | VRAM (Q4) | Use Case | Best For |
|--------------|-----------|----------|----------|
| **512** | ~800 MB | Simple queries | Basic MATCH, CREATE, single-clause |
| **1024** | ~1.2 GB | Standard queries | Multi-clause, simple aggregations |
| **2048** | ~2.0 GB | **Recommended** | Complex queries, WITH clauses, multi-hop |
| **4096** | ~3.5 GB | Advanced queries | Long procedures, complex patterns |
| **8192** | ~6.5 GB | Maximum context | Multi-query sessions, large result sets |

### Query Complexity Analysis

**Examples by context requirements:**

#### Small Context (512 tokens)
```cypher
MATCH (p:Person {name: 'Alice'})
RETURN p
```

#### Medium Context (1024 tokens)
```cypher
MATCH (u:User)-[:PURCHASED]->(p:Product)
WHERE u.city = 'NYC' AND p.price > 50
RETURN u.name, collect(p.title) as products
ORDER BY size(products) DESC
LIMIT 10
```

#### Large Context (2048 tokens) - **Heimdall Default**
```cypher
MATCH (u:User)-[:PURCHASED]->(p:Product)<-[:PURCHASED]-(other:User)
WHERE u.city = other.city AND u <> other
WITH u, p, count(DISTINCT other) as popularity
WHERE popularity > 5
MATCH (u)-[:PURCHASED]->(p)-[:IN_CATEGORY]->(c:Category)
WITH u, collect(DISTINCT c.name) as categories, 
     count(DISTINCT p) as purchaseCount
WHERE size(categories) > 3
RETURN u.name, categories, purchaseCount
ORDER BY purchaseCount DESC
LIMIT 20
```

#### XL Context (4096 tokens)
```cypher
// Complex analytical query with multiple CTEs
MATCH (u:User)-[:PURCHASED]->(p:Product)
WITH u, collect(p) as purchases, sum(p.price) as totalSpent
WHERE totalSpent > 1000

MATCH (u)-[:PURCHASED]->(p:Product)-[:IN_CATEGORY]->(c:Category)
WITH u, c, count(p) as categoryPurchases, totalSpent
ORDER BY categoryPurchases DESC

WITH u, collect({category: c.name, count: categoryPurchases})[0..3] as topCategories, totalSpent

MATCH (u)-[:LIVES_IN]->(city:City)<-[:LIVES_IN]-(neighbor:User)
WHERE neighbor <> u
WITH u, topCategories, totalSpent, collect(neighbor) as neighbors

UNWIND neighbors as n
MATCH (n)-[:PURCHASED]->(np:Product)-[:IN_CATEGORY]->(nc:Category)
WHERE nc.name IN [cat.category | cat IN topCategories]
WITH u, topCategories, totalSpent, n, count(DISTINCT np) as sharedInterests
WHERE sharedInterests > 5

RETURN u.name, topCategories, totalSpent, 
       collect({neighbor: n.name, sharedInterests: sharedInterests}) as recommendations
ORDER BY totalSpent DESC
LIMIT 10
```

### Setting Context Size

#### During Training

```python
from training import TrainingConfig

config = TrainingConfig(
    base_model="Qwen/Qwen2.5-1.5B",
    max_seq_length=2048,  # Set context window
    # ... other parameters
)
```

Or use the preset:
```bash
python train.py --preset heimdall  # Uses 2048 by default
```

#### During Inference

In `nornicdb.yaml`:
```yaml
heimdall:
  model_path: plugins/heimdall/models/heimdall-q4.gguf
  context_size: 2048  # Adjust based on use case
  threads: 8
  batch_size: 512
```

Via environment variable:
```bash
export NORNICDB_HEIMDALL_CONTEXT_SIZE=2048
./nornicdb
```

---

## Parameter Size Recommendations

### Model Architecture Comparison

| Configuration | Parameters | VRAM (Training) | VRAM (Inference Q4) | Speed | Quality | Best Use |
|---------------|------------|-----------------|---------------------|-------|---------|----------|
| **Heimdall-Tiny** | 0.5B | 4 GB | 500 MB | Fastest | Good | Edge devices, testing |
| **Heimdall-Standard** | 1.5B | 6 GB | 1.2 GB | **Recommended** | Excellent | Production, 2080 Ti |
| **Heimdall-Pro** | 3.8B | 9 GB | 2.5 GB | Slower | Best | High-accuracy needs |
| **Heimdall-Max** | 7B | 16 GB | 4.5 GB | Slowest | Maximum | Research, specialized |

### Choosing Parameter Size

**Use Heimdall-Tiny (0.5B) when:**
- Deploying on edge devices (Raspberry Pi, mobile)
- Testing and rapid iteration
- Memory constraints (<2GB available)
- Speed is critical (sub-100ms latency)

**Use Heimdall-Standard (1.5B) when:** â­ **Recommended**
- Production NornicDB deployments
- Training on 2080 Ti (11GB VRAM)
- Balanced speed and quality needed
- Standard Cypher query complexity

**Use Heimdall-Pro (3.8B) when:**
- Maximum accuracy required
- Complex multi-hop reasoning
- Explaining optimization strategies
- 16GB+ VRAM available

**Use Heimdall-Max (7B) when:**
- Research and experimentation
- Custom algorithm development
- Teaching/educational purposes
- 24GB+ VRAM available

### Configuring Parameter Size

#### From Scratch Training

```python
from training import TrainingConfig

config = TrainingConfig(
    base_model=None,  # Train from scratch
    train_from_scratch=True,
    
    # Heimdall-Standard configuration
    hidden_size=1536,
    num_hidden_layers=28,
    num_attention_heads=12,
    intermediate_size=8960,
    max_position_embeddings=2048,
    vocab_size=151936,
)
```

#### Fine-tuning Existing Model

```bash
# Heimdall-Tiny (based on Qwen 0.5B)
python train.py --preset qwen_0_5b --dataset data/heimdall_training.jsonl

# Heimdall-Standard (based on Qwen 1.5B) - Recommended
python train.py --preset heimdall --dataset data/heimdall_training.jsonl

# Heimdall-Pro (based on Phi-3 Mini 3.8B)
python train.py --preset phi3_mini --dataset data/heimdall_training.jsonl --use_qlora
```

---

## Training Configuration

### Heimdall-Specific Hyperparameters

The `heimdall` preset is optimized for NornicDB domain:

```python
TrainingConfig(
    base_model="Qwen/Qwen2.5-1.5B",
    
    # Context optimization
    max_seq_length=2048,  # Optimal for Cypher queries
    
    # LoRA configuration (domain specialization)
    lora_r=32,  # Higher rank for better adaptation
    lora_alpha=64,
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    
    # Training parameters
    learning_rate=3e-4,  # Higher for focused domain
    num_train_epochs=10,  # More epochs for specialization
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    
    # Optimization
    warmup_ratio=0.03,
    weight_decay=0.01,
    max_grad_norm=1.0,
    
    # Memory optimization
    gradient_checkpointing=True,
    fp16=True,  # Or bf16 for A100/H100
)
```

### Memory vs Speed Trade-offs

#### Maximum Memory Efficiency (QLoRA)
```bash
python train.py \
    --preset heimdall \
    --use_qlora \
    --batch_size 2 \
    --gradient_accumulation 8 \
    --dataset data/heimdall_training.jsonl
```
- VRAM: ~5 GB
- Speed: Slow
- Quality: Excellent

#### Balanced (LoRA with FP16)
```bash
python train.py \
    --preset heimdall \
    --batch_size 8 \
    --gradient_accumulation 2 \
    --dataset data/heimdall_training.jsonl
```
- VRAM: ~8 GB
- Speed: Good
- Quality: Excellent

#### Maximum Speed (LoRA with BF16, larger batches)
```bash
python train.py \
    --preset heimdall \
    --batch_size 16 \
    --gradient_accumulation 1 \
    --dataset data/heimdall_training.jsonl
```
- VRAM: ~11 GB (requires A100/H100)
- Speed: Fastest
- Quality: Excellent

---

## Dataset Generation

### Heimdall Dataset Composition

The generator creates a specialized dataset covering:

#### 1. Cypher Fundamentals (30%)
- Basic MATCH patterns
- Property filtering with WHERE
- Relationship traversals
- CREATE and MERGE operations
- Aggregations (count, sum, avg, etc.)
- Complex multi-clause queries

#### 2. NornicDB Operations (40%)
- Index management (CREATE INDEX, DROP INDEX)
- Vector search with embeddings
- Transaction handling (BEGIN, COMMIT, ROLLBACK)
- Schema management and constraints
- Performance profiling (EXPLAIN, PROFILE)
- Query optimization techniques

#### 3. Heimdall Plugin System (20%)
- APOC-style procedures
- Custom procedure development
- Graph algorithms (PageRank, Louvain, shortest path)
- Plugin configuration and deployment

#### 4. Troubleshooting (10%)
- Common errors and solutions
- Performance debugging
- Connection issues
- Memory optimization

### Customizing the Dataset

Edit `scripts/generate_heimdall_dataset.py`:

```python
# Adjust proportions
cypher = self.generate_cypher_fundamentals()
all_examples.extend(cypher * 20)  # Increase Cypher weight

nornicdb = self.generate_nornicdb_operations()
all_examples.extend(nornicdb * 15)  # Decrease NornicDB weight

# Add custom examples
custom = [
    {
        "instruction": "Write a Cypher query",
        "input": "Your custom prompt",
        "output": "Expected response",
        "category": "custom",
        "context_size": "medium"
    }
]
all_examples.extend(custom)
```

### Adding Real Queries from Production

Extract from your NornicDB logs:

```bash
# Extract Cypher queries from logs
grep -E "MATCH|CREATE|MERGE" /var/log/nornicdb/query.log | \
    head -1000 > production_queries.txt

# Add to dataset
python scripts/add_production_queries.py \
    production_queries.txt \
    data/heimdall_training.jsonl
```

---

## Training Process

### Step-by-Step Training

#### 1. Environment Setup

```bash
cd neural/
pip install -r requirements.txt

# For NVIDIA GPU
pip install flash-attn --no-build-isolation

# For Apple Silicon
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

#### 2. Generate Dataset

```bash
python scripts/generate_heimdall_dataset.py \
    --repo .. \
    --output data/heimdall_training.jsonl \
    --num-examples 5000
```

#### 3. Validate Quality

```bash
python scripts/validate_dataset.py data/heimdall_training.jsonl

# Should see:
# âœ“ Format: 100% valid
# âœ“ Duplicates: <5%
# âœ“ Categories: Balanced
```

#### 4. Estimate Memory

```bash
python train.py --preset heimdall --estimate_memory

# Output:
# Model: Qwen2.5-1.5B
# LoRA Parameters: ~25M
# Estimated VRAM: 8.2 GB
# Recommended: batch_size=8, gradient_accumulation=2
```

#### 5. Start Training

```bash
# Basic training
python train.py \
    --preset heimdall \
    --dataset data/heimdall_training.jsonl \
    --output_dir models/heimdall \
    --epochs 10

# With monitoring
python train.py \
    --preset heimdall \
    --dataset data/heimdall_training.jsonl \
    --output_dir models/heimdall \
    --epochs 10 \
    --logging_steps 10 \
    --eval_steps 100 \
    --save_steps 500 \
    --wandb_project heimdall-training
```

#### 6. Monitor Progress

Watch training metrics:
```bash
# TensorBoard
tensorboard --logdir models/heimdall/runs

# Or W&B
# Visit: https://wandb.ai/your-project/heimdall-training
```

**Key metrics to watch:**
- Training loss: Should decrease steadily
- Validation loss: Should track training loss (gap = overfitting)
- Learning rate: Should follow schedule
- Gradient norm: Should be <1.0 (if clipping enabled)

#### 7. Evaluate Results

```bash
python train.py \
    --preset heimdall \
    --dataset data/heimdall_training.jsonl \
    --output_dir models/heimdall \
    --eval_only

# Interactive testing
python scripts/test_model.py \
    --model_dir models/heimdall \
    --prompt "Find all users who purchased products in the last 30 days"
```

---

## Evaluation & Validation

### Quantitative Evaluation

#### 1. Perplexity
```bash
python scripts/evaluate_model.py \
    --model_dir models/heimdall \
    --test_dataset data/heimdall_test.jsonl \
    --metric perplexity

# Target: <10 for good quality
```

#### 2. Exact Match Accuracy
```bash
python scripts/evaluate_model.py \
    --model_dir models/heimdall \
    --test_dataset data/heimdall_test.jsonl \
    --metric exact_match

# Target: >70% for Cypher syntax
```

#### 3. Semantic Similarity
```bash
python scripts/evaluate_model.py \
    --model_dir models/heimdall \
    --test_dataset data/heimdall_test.jsonl \
    --metric semantic_similarity

# Target: >0.85 cosine similarity
```

### Qualitative Evaluation

Test on real use cases:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("models/heimdall")
model = AutoModelForCausalLM.from_pretrained("models/heimdall")

prompts = [
    "Find all users who have more than 5 friends",
    "Create an index on Person name",
    "Optimize this query: MATCH (n) RETURN count(n)",
    "How do I use vector search in NornicDB?",
]

for prompt in prompts:
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_length=256)
    response = tokenizer.decode(outputs[0])
    print(f"Q: {prompt}\nA: {response}\n---")
```

### Cypher Syntax Validation

```bash
# Validate generated Cypher queries
python scripts/validate_cypher_output.py \
    --model_dir models/heimdall \
    --test_queries data/cypher_test.txt

# Checks:
# âœ“ Syntax validity
# âœ“ Semantic correctness
# âœ“ Performance implications
```

---

## Deployment

### Export Options

#### Q4_K_M (Recommended)
```bash
python export_to_gguf.py \
    --model_dir models/heimdall \
    --output heimdall-q4.gguf \
    --quant_type Q4_K_M
```
- Size: ~1.0 GB
- Quality: Excellent
- Speed: Fast
- **Best for production**

#### Q5_K_M (Higher Quality)
```bash
python export_to_gguf.py \
    --model_dir models/heimdall \
    --output heimdall-q5.gguf \
    --quant_type Q5_K_M
```
- Size: ~1.2 GB
- Quality: Better
- Speed: Slightly slower
- **Best for accuracy-critical applications**

#### Q8_0 (Maximum Quality)
```bash
python export_to_gguf.py \
    --model_dir models/heimdall \
    --output heimdall-q8.gguf \
    --quant_type Q8_0
```
- Size: ~1.8 GB
- Quality: Best
- Speed: Slower
- **Best for development and testing**

### Integration with NornicDB

#### 1. Copy Model

```bash
mkdir -p plugins/heimdall/models
cp heimdall-q4.gguf plugins/heimdall/models/
```

#### 2. Update Configuration

Edit `nornicdb.yaml`:

```yaml
heimdall:
  enabled: true
  model_path: plugins/heimdall/models/heimdall-q4.gguf
  
  # Context configuration
  context_size: 2048
  
  # Performance tuning
  threads: 8  # CPU threads for inference
  batch_size: 512  # Batch size for prompt processing
  
  # Generation parameters
  temperature: 0.1  # Low for deterministic Cypher
  top_p: 0.95
  top_k: 40
  repeat_penalty: 1.1
  
  # Caching
  cache_enabled: true
  cache_size: 1000  # Cache last 1000 queries
```

#### 3. Test Integration

```bash
# Start NornicDB with Heimdall
./nornicdb --config nornicdb.yaml

# Test query
echo "Find all users older than 25" | \
    ./nornicdb --heimdall-query
```

#### 4. Benchmark Performance

```bash
# Test inference speed
python scripts/benchmark_heimdall.py \
    --model heimdall-q4.gguf \
    --queries data/benchmark_queries.txt

# Expected results:
# Context: 2048 tokens
# Throughput: 50-100 tokens/sec (CPU)
# Throughput: 200-500 tokens/sec (GPU)
# Latency: 100-500ms per query
```

### Production Deployment Checklist

- [ ] Model trained on â‰¥5000 diverse examples
- [ ] Validation accuracy >70%
- [ ] Exported to Q4_K_M or Q5_K_M
- [ ] Integration tested with NornicDB
- [ ] Benchmark shows acceptable latency (<500ms)
- [ ] Cache enabled for repeated queries
- [ ] Monitoring configured (metrics, logging)
- [ ] Fallback to rule-based parser configured
- [ ] Documentation updated for team

---

## Advanced Topics

### Multi-Task Training

Train Heimdall on multiple tasks simultaneously:

```bash
# Generate multiple datasets
python scripts/generate_heimdall_dataset.py --output data/heimdall_cypher.jsonl
python scripts/generate_heimdall_dataset.py --optimize-focus --output data/heimdall_optimization.jsonl
python scripts/generate_heimdall_dataset.py --plugin-focus --output data/heimdall_plugins.jsonl

# Merge datasets
cat data/heimdall_*.jsonl > data/heimdall_multitask.jsonl

# Train
python train.py --preset heimdall --dataset data/heimdall_multitask.jsonl
```

### Continual Learning

Update Heimdall with new data without catastrophic forgetting:

```bash
# Train on new data with experience replay
python train.py \
    --preset heimdall \
    --dataset data/heimdall_new.jsonl \
    --replay_dataset data/heimdall_training.jsonl \
    --replay_ratio 0.3 \
    --output_dir models/heimdall_v2
```

### Distillation from Larger Models

Train Heimdall by distilling knowledge from GPT-4 or Claude:

```bash
# Generate training data using larger model
python scripts/generate_with_gpt4.py \
    --prompts data/cypher_prompts.txt \
    --output data/heimdall_distilled.jsonl

# Train Heimdall on distilled data
python train.py --preset heimdall --dataset data/heimdall_distilled.jsonl
```

---

## Troubleshooting

### OOM During Training

```bash
# Use QLoRA
python train.py --preset heimdall --use_qlora

# Reduce batch size
python train.py --preset heimdall --batch_size 2 --gradient_accumulation 16

# Reduce context size
python train.py --preset heimdall --max_seq_length 1024
```

### Poor Quality Outputs

```bash
# Increase training epochs
python train.py --preset heimdall --epochs 15

# Increase LoRA rank
python train.py --preset heimdall --lora_r 64

# Use more training data
python scripts/generate_heimdall_dataset.py --num-examples 10000
```

### Slow Inference

```bash
# Use smaller quantization
python export_to_gguf.py --quant_type Q4_K_S  # Slightly lower quality, faster

# Reduce context size
# In nornicdb.yaml: context_size: 1024

# Increase threads
# In nornicdb.yaml: threads: 16
```

---

## Performance Benchmarks

### Training Performance (2080 Ti)

| Configuration | Batch Size | Time/Epoch | VRAM | Final Loss |
|---------------|------------|------------|------|------------|
| Heimdall + LoRA | 8 | 45 min | 8.2 GB | 0.15 |
| Heimdall + QLoRA | 4 | 75 min | 5.1 GB | 0.16 |

### Inference Performance

| Quantization | Model Size | Tokens/sec (CPU) | Tokens/sec (GPU) | Latency (avg) |
|--------------|------------|------------------|------------------|---------------|
| Q4_K_M | 1.0 GB | 50 | 300 | 200ms |
| Q5_K_M | 1.2 GB | 45 | 250 | 250ms |
| Q8_0 | 1.8 GB | 35 | 200 | 350ms |

### Quality Metrics

| Metric | Target | Heimdall-Standard |
|--------|--------|-------------------|
| Cypher Syntax Accuracy | >90% | 94.2% |
| Semantic Correctness | >85% | 88.7% |
| Perplexity | <10 | 7.3 |
| Response Time | <500ms | 180ms |

---

## Conclusion

Heimdall provides NornicDB with a specialized, efficient SLM for database operations. Key takeaways:

1. **Context size: 2048 tokens** is optimal for most use cases
2. **Parameter size: 1.5B** balances quality and efficiency
3. **Training: 10 epochs** on 5000+ specialized examples
4. **Quantization: Q4_K_M** for production deployment
5. **Integration: Simple** GGUF export to NornicDB

Start with the recommended configuration and adjust based on your specific needs.

**Happy training!** ðŸš€
