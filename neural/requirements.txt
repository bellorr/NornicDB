# Core Training Dependencies
torch>=2.1.0
transformers>=4.36.0
datasets>=2.16.0
peft>=0.7.0  # Parameter-Efficient Fine-Tuning (LoRA)
bitsandbytes>=0.41.0  # For QLoRA 4-bit training
accelerate>=0.25.0  # Multi-GPU and optimization

# Data Processing
pandas>=2.0.0
numpy>=1.24.0
tqdm>=4.65.0
PyYAML>=6.0.0  # For configuration files
pypdf>=4.0.0

# Model Export & Conversion
safetensors>=0.4.0
sentencepiece>=0.1.99  # For tokenizer
protobuf>=4.25.0

# Training Utilities
tensorboard>=2.15.0  # Training visualization
wandb>=0.16.0  # Optional: Weights & Biases integration
scikit-learn>=1.3.0  # Evaluation metrics

# GGUF Conversion
# Note: Requires llama.cpp Python bindings
# Install with: pip install llama-cpp-python
# Or build from source for GPU support

# Optional: Flash Attention 2 (significant speedup)
# Requires CUDA 11.8+
# Install with: pip install flash-attn --no-build-isolation

# Development
pytest>=7.4.0
black>=23.12.0
isort>=5.13.0
